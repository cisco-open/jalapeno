{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Project Jalapeno","text":""},{"location":"#a-cloud-native-infrastructure-platform-to-enable-development-of-network-services","title":"A cloud-native infrastructure platform to enable development of network services","text":"<p>Project Jalapeno combines existing open source tools with services and capabilities we've developed into an infrastructure platform intended to enable development of SDN control plane applications.  Our goal: give developers the ability to quickly and easily build SDN control applications on top of a common data collection and warehousing infrastructure (Jalapeno).</p> <p>This website provides documentation and shell scripts to install Jalapeno's component images.</p> <p></p> <ul> <li> <p> Learn About Jalapeno</p> <p>Read about the project components and architecture.</p> <p> About</p> </li> <li> <p> Setup &amp; Install</p> <p>Set up your own instance of Jalapeno &amp; start collecting data!</p> <p> Getting Started</p> </li> </ul> <p></p>"},{"location":"about/","title":"About Jalapeno","text":"<p>This section contains details about the overall design of Jalapeno, including notes about the different components of the project.</p>"},{"location":"about/#high-level-architecture","title":"High level architecture","text":"<p>The diagram below provides an general idea of the architecture behind the project &amp; intended interaction between components. Within the pages of this section, We'll dive into each of these components separately to describe their functions.</p> <p></p>"},{"location":"about/#platform-overview","title":"Platform Overview","text":"<p>SDN is a Database Problem</p>"},{"location":"about/#platform-overview_1","title":"Platform Overview","text":"<p>At the heart of Jalapeno is the concept that many SDN use cases really involve the creation of virtual topologies whose type and characteristics are driven by dataplane encapsulations and other meta data (Tunnel Building) . And network topologies (whether real or virtual) can be modeled as graphs. Thus, if we think in terms of graphs, we can address any topological use case as an exercise in database mapping. With this framework in mind, Jalapeno has the theoretical ability to address any kind of virtual topology use case, for example:</p> <ul> <li>Internal Traffic Engineering (TE) - engineered tunnels traversing a network under common management (BGP-LS use cases - see note below)</li> <li>Egress Peer Engineering (EPE) - engineered tunnels sending traffic out a specific egress router/interface to an external network</li> <li>SD-WAN - various combinations of TE and EPE</li> <li>VPN Overlays / Segmentation - engineered tunnels creating point-to-point or multipoint overlay virtual networks</li> <li>Network Slicing - add some SLA to VPN overlays</li> <li>VPN Overlays with TE, EPE, SDWAN - various combinations of Overlay and TE services</li> <li>Service Chaining - engineered tunnels, potentially a series of them, linked together via or seamlessly traversing midpoint service nodes</li> <li>Service Meshes - generally a service-mesh is more of a layer-7 graph, but the mesh could be augmented with some layer-3 topological service as well</li> </ul>"},{"location":"about/#some-project-principles-and-goals","title":"Some project principles and goals","text":"<ul> <li>Use-case, topology, and endpoint agnostic - The host, hypervisor, CNI, or other endpoint may be the control/encapsulation point (linux, fd.io, eBPF, etc.)</li> <li>Give applications the ability to directly choose their network SDN service or SLA</li> <li>Enable development of an ecosystem of SDN control applications, tools, and capabilities</li> <li>Modular, extensible, microservice architecture</li> <li>Emphasize the use of APIs over Protocols for greater agility</li> </ul>"},{"location":"about/#key-components","title":"Key Components","text":"<p>Jalapeno is comprised of a series of microservices which can be summarized as:</p> <ul> <li> <p>Collectors</p> <ul> <li>Capture network topology and performance data and feed the data to Kafka.  Eventually we wish to incorporate application and server/host performance data as well.  The collection stack also includes Influx TSDB and Grafana for data visualization.</li> </ul> </li> <li> <p>Processors</p> <ul> <li>Data Processors</li> <li>Jalapeno has two classes of processors:<ul> <li>Base data processors: Parse topology and performance data coming off Kafka and populate the Influx TSDB and base data collections in the Arango graph database.  The Topology and Telegraf pods are base processors.</li> <li>Virtual Topology or Edge processors: Mine the graph and TSDB data collections, then populate virtual topology Edge collections in the graph DB.  Linkstate-edge is an one such processor.</li> </ul> </li> </ul> </li> <li> <p>Infrastructure</p> <ul> <li>Kafka: used as a message bus between data collectors and processors. Can also be leveraged by outside systems that wish to subscribe to Jalapeno Kafka topics.</li> <li>Arango Graph Database: used for topology modeling and as a document or key-value store. </li> <li>Influx Time Series Database: used for collection and warehousing of streaming telemetry data from Telegraf/gNMI.</li> </ul> </li> <li> <p>API - expose Jalapeno's virtual topology data for application consumption (API-GW is under construction)</p> <ul> <li>An implementation focusing on fetching topology and telemetry data from Jalape\u00f1o can be found in a separate GitHub organisation</li> </ul> </li> <li> <p>Control Plane Applications - SDN control applications that mine the graph and time-series databases for the label stack or SRv6 SRH data needed to execute topology or traffic engineering use cases. </p> </li> </ul> <p>In an example use case, an end user or application would like to send their backup/background traffic to its destination via the least utilized path. The intent would be to preserve more capacity on the routing protocol's chosen best path. Jalapeno responds to the request with a segment routing label stack that, when appended to outbound packets, will steer traffic over the least utilized path. The app then re-queries Jalapeno every 10 seconds and updates the SR label stack should the least utilized path change.</p> <p>Jalapeno's kubernetes architecture make it inherently extensible, and we imagine the number of collectors, graphDB virtual topology use cases, and control plane applications to expand significantly as our community grows.</p>"},{"location":"about/#a-note-on-bgp-ls","title":"A Note on BGP-LS","text":"<p>The key to developing and supporting virtual topology use cases is the programmatic acquisition of topology data. Traditional SDN-TE platforms focus on Internal-TE and therefore leverage BGP-LS. With Jalapeno we wish to eventually support all the above categories of use case, and therefore we use BGP Monitoring Protocol (BMP) and leverage the GoBMP collector.</p> <p>BMP provides a superset of topology data, including:</p> <ul> <li>BGP-LS topology data</li> <li>iBGP and eBGP IPv4, IPv6, and labeled unicast topology data</li> <li>BGP VPNv4, VPNv6, and EVPN topology data</li> </ul>"},{"location":"about/collectors/","title":"Jalapeno Collectors","text":"<p>Jalapeno Collectors are responsible for collecting topology and performance data from the network.</p> <p>Any Jalapeno infrastructure component that brings data into the Jalapeno cluster from the network is considered a Collector.</p>"},{"location":"about/collectors/#gobmp-collector","title":"GoBMP Collector","text":"<p>To collect topology-related data, Jalapeno currently uses the golang implementation of OpenBMP, or \"GoBMP\". Devices can be configured to send BMP data to the cluster via the GoBMP Collector, which is deployed as a <code>StatefulSet</code> in Kubernetes. Inside the <code>/install/collectors/gobmp</code> directory, the <code>gobmp-collector.yaml</code> file defines the pod to be built using the latest GoBMP image.</p> <p>Thus, Jalapeno has a GoBMP Collector pod running in the cluster that serves as the ingress point for all topology data from the devices. Once the data arrives from the network to the GoBMP pod, the data is then forwarded to Kafka for the next level of data processing (see Processors).</p> <p>You can learn more about GoBMP and the way GoBMP organizes topology data here.</p>"},{"location":"about/collectors/#telegraf-ingress-collector","title":"Telegraf-Ingress Collector","text":"<p>To collect performance-related data, Jalapeno uses Telegraf.</p> <p>Devices are configured to send telemetry data to the cluster via the Telegraf-Ingress Collector, which is deployed as a Deployment in Kubernetes. Inside the <code>/install/collectors/telegraf-ingress</code> directory, the <code>telegraf_ingress_dp.yaml</code> file defines the pod to be built using the latest Telegraf image, and then loads its configuration from the <code>telegraf_ingress_cfg.yaml</code> file.</p> <p>Thus, Jalapeno has an Telegraf-Ingress Collector pod running in the cluster that serves as the ingress point for all performance data from the devices. As defined in the <code>telegraf_ingress_cfg.yaml</code> file, the data is then forwarded to Kafka into the <code>jalapeno.telemetry</code> topic, which can be queried by Jalapeno's performance processors (see Processors).</p> <p>Note</p> <p>This collector is called Telegraf-Ingress specifically because there is also a Telegraf-Egress processor. The egress processor forwards the data from Kafka to InfluxDB further down Jalapeno's data pipeline.</p>"},{"location":"about/infrastructure/","title":"Jalapeno Infrastructure","text":"<p>Jalapeno has the following components that create its infrastructure: Kafka, ArangoDB, InfluxDB, Grafana, and Telegraf.</p> <p>All infrastructure components reside in the Jalapeno Kubernetes cluster and are deployed using the <code>/install/infra/deploy_infrastructure.sh</code> script.</p> <p>Each Jalapeno infrastructure component defines its deployment in the respective YAML files. These files allow for the configuration of services, deployments, config-maps, node-ports and other Kubernetes components.</p>"},{"location":"about/infrastructure/#kafka","title":"Kafka","text":"<p>Kafka is Jalapeno's message bus and core data handler.</p> <p>Jalapeno's Kafka instance handles two main types of data: BMP data (topology information) supplied by the GoBMP collector, and telemetry data (network metrics) supplied by Telegraf. Jalapeno Processors are responsible for reading and restructuring the data, and for inferring relevant metrics from the data.</p> <p>BMP data is organized into Kakfa topics such as <code>gobmp.parsed.peer</code> and <code>gobmp.parsed.ls_node</code>. These topics are further parsed to create representations of the network topology using the Topology Processor.</p> <p>Telemetry data is collected in the <code>jalapeno.telemetry</code> topic. Data in this topic is pushed into Telegraf (a telemetry consumer), and onwards into InfluxDB.</p> <p>Kafka is deployed using <code>kubectl</code>, as seen in the <code>deploy_infrastructure.sh</code> script. The configurations for Kafka's deployment are in the YAML files in the <code>jalapeno/infra/kafka/</code> directory.</p>"},{"location":"about/infrastructure/#arangodb","title":"ArangoDB","text":"<p>ArangoDB is Jalapeno's graph database.</p> <p>Jalapeno Processors parse through data in Kafka, then create various collections in ArangoDB. These collections represent both the network's topology and its current state. For example, the Topology Processor parses BMP messages that have been streamed to Kafka and builds out collections such as \"ls_node\" and \"l3vpn_prefix_v4\" in Jalapeno's ArangoDB instance. These collections, in conjunction with ArangoDBs rapid graphical traversals make it easy to model topologies and make path calculations.</p> <p>ArangoDB is deployed using <code>kubectl</code>, as seen in the <code>deploy_infrastructure.sh</code> script. The configurations for ArangoDB's deployment are in the YAML files in the <code>jalapeno/infra/arangodb/</code> directory.  </p> <p>To access ArangoDB's UI, log in at <code>&lt;server_ip&gt;:30852</code>, using credentials <code>root/jalapeno</code>. In the list of DBs, select <code>jalapeno</code>.</p>"},{"location":"about/infrastructure/#influxdb","title":"InfluxDB","text":"<p>InfluxDB is Jalapeno's time-series database.</p> <p>Telemetry data is parsed and passed from Kafka into InfluxDB using a telemetry consumer (Telegraf).</p> <p>The data stored in InfluxDB can be queried by Processors and by the ArangoDB Jalapeno API. These queries construct and derive relevant metrics. For example, a processor could generate rolling-averages of bytes sent out of a router's interface, which would be used to simulate link utilization.</p> <p>Using InfluxDB as a historical data-store, Jalapeno Processors can also infer trends based on historical analysis. Processors and even applications can determine whether instantaneous measurements are extreme anomalies, and can enable requests for any number of threshold-based reactions.</p> <p>InfluxDB is deployed using <code>kubectl</code>, as seen in the <code>deploy_infrastructure.sh</code> script. The configurations for InfluxDB's deployment are in the YAML files in the <code>jalapeno/infra/influxdb/</code> directory.  </p> <p>To access InfluxDB via Kubernetes, enter the pod's terminal and run:</p> <pre><code>influx\nauth root jalapeno\nuse mdt_db\nshow series\n</code></pre>"},{"location":"about/infrastructure/#grafana","title":"Grafana","text":"<p>The Jalapeno installation package includes a Grafana instance for creating dashboards and metric visualizations.</p> <p>Loaded with InfluxDB as its data-source, Grafana has various graphical representations of the network, including historical bandwidth usage, historical latency metrics, and more.</p> <p>Grafana is deployed using <code>kubectl</code>, as seen in the <code>deploy_infrastructure.sh</code> script. The configurations for Grafana's deployment are in the various YAML files in the <code>jalapeno/infra/grafana/</code> directory.</p> <p>To access Grafana's UI, log in at <code>&lt;server_ip&gt;:30300</code>, using credentials <code>root/jalapeno</code>.</p> <p>A pair of example dashboard configurations can be found here:</p> <p>Interface Egress Stats</p> <p>and here:</p> <p>Interface Ingress Stats</p>"},{"location":"about/infrastructure/#telegraf-egress","title":"Telegraf-Egress","text":"<p>Telegraf is a telemetry consumer and forwarder.</p> <p>Within the Jalapeno infrastructure, the Telegraf-Egress deployment of Telegraf consumes data from Kafka and forwards this data to InfluxDB.</p> <p>Telegraf-Egress is deployed using <code>kubectl</code>, as seen in the <code>deploy_infrastructure.sh</code> script. The configurations for the Pipeline deployments are in the various YAML files in the <code>jalapeno/infra/telegraf-egress/</code> directory.</p> <p>Note</p> <p>There exists another Telegraf instance (Telegraf-Ingress) that is part of Jalapeno's Collectors. The ingress instance consumes data from devices directly before forwarding the data to Kafka.</p>"},{"location":"about/processors/","title":"Jalapeno Data Processors","text":"<p>Jalapeno's Data Processors are responsible for organizing, parsing, and analysing network topology and performance data.</p> <p>Any Jalapeno Infrastructure component with data is considered a source for a Processor.</p>"},{"location":"about/processors/#topology-processor","title":"Topology Processor","text":"<p>BGP speakers send BMP data feeds to GoBMP, which then passes the data to Kafka.  The Topology Processor subscribes to Kafka's BMP topics in order to create topology representations in ArangoDB.</p> <p>Collections created using this service are considered base-collections. These base-collections have no inference of relationships between network elements, or of any metrics - they are organized collections of individual GoBMP messages.</p> <p>For example, the Topology processor creates the LSNode collection and the LSLink collection directly from GoBMP BGP-LS message data.</p> <p>The configuration for topology deployment is in \"topology_dp.yaml\" in the topology directory.</p>"},{"location":"about/processors/#other-processors","title":"Other Processors","text":"<p>Currently the project is bundled with a limited set of processors. However, other processors can be found in this repository which may offer additional functionality to Jalapeno.</p>"},{"location":"arango/example-queries-2/","title":"Example queries 2","text":""},{"location":"arango/example-queries-2/#example-queries-part-2","title":"Example Queries part 2","text":""},{"location":"arango/example-queries-2/#note-you-can-use-golang-style-to-comment-lines-out","title":"Note: you can use golang-style // to comment lines out","text":"<p>for v, e in outbound shortest_path 'sr_node/2_0_0_0000.0000.0025' TO 'unicast_prefix_v4/10.10.3.0_24_10.0.0.29' sr_topology return  { prefix: v.prefix, name: v.name, sid: e.srv6_sid, latency: e.latency }</p> <p>for v, e in outbound shortest_path 'sr_node/2_0_0_0000.0000.0025' to 'unicast_prefix_v4/10.10.3.0_24_10.0.0.29' sr_topology OPTIONS {weightAttribute: 'latency' } return  { prefix: v.prefix, name: v.name, sid: e.srv6_sid, latency: e.latency }</p> <p>for v, e, p IN 1..6 outbound 'sr_node/2_0_0_0000.0000.0025' sr_topology OPTIONS {uniqueVertices: \"path\", bfs: true} FILTER v._id == 'unicast_prefix_v4/10.10.3.0_24_10.0.0.29' return { path: p.edges[].remote_node_name, sid: p.edges[].srv6_sid, country_list: p.edges[].country_codes[], latency: sum(p.edges[].latency), percent_util_out: avg(p.edges[].percent_util_out)} </p> <p>for v, e in outbound shortest_path 'sr_node/2_0_0_0000.0000.0025' to 'unicast_prefix_v4/10.10.3.0_24_10.0.0.29' sr_topology OPTIONS {weightAttribute: 'latency' }  return  { prefix: v.prefix, name: v.name, sid: e.srv6_sid, latency: e.latency, cc: e.country_codes }</p> <p>for p in outbound k_shortest_paths  'sr_node/2_0_0_0000.0000.0025' to 'unicast_prefix_v4/10.10.3.0_24_10.0.0.29' sr_topology  filter p.edges[].country_codes !like \"%FRA%\" return { path: p.edges[].remote_node_name, sid: p.edges[].srv6_sid, country_list: p.edges[].country_codes[], latency: sum(p.edges[].latency), percent_util_out: avg(p.edges[*].percent_util_out)} </p> <p>Basic queries</p> <p>for l in ls_srv6_sid_edge return l for l in ls_prefix return l for l in ls_node_edge return l  for l in ls_node_edge return l for l in ls_node_edge filter l.protocol_id == 7 return l._key for p in unicast_prefix_v4 filter p._key == \"10.71.8.0_22_10.71.0.1\" return p FOR d IN peer filter d.remote_bgp_id == \"10.0.0.71\" filter d.remote_ip == \"10.71.0.1\" return d</p> <p>for l in UnicastPrefixV4 filter l.peer_ip == \"10.2.2.3\" return { UnicastPrefixV4: l, LSLink: (for s in LSLink filter s.remote_link_ip == \"10.2.2.3\"return s)}</p> <p>for u in UnicastPrefixV4 for l in LSLink filter u.peer_ip == l.remote_link_ip filter u.peer_ip == \"10.71.1.1\"  return {u: u._id, l: l.remote_link_ip}</p> <p>FOR d IN peer filter d.remote_ip == \"10.71.0.1\" FOR l in ls_link  filter d.remote_ip == l.remote_link_ip  return d</p> <p>for d in peer filter d.remote_ip == \"10.1.40.3\" for l in ls_link  filter d.remote_ip == l.remote_link_ip  return { d, l }</p> <p>//FOR v, e, p IN 1..10 ANY \"ls_node/2_0_0_0000.0000.0019\" lsv4_edge      //FILTER v._id == \"ls_node/2_0_0_0000.0000.0009\"    //  FILTER v._id == \"unicast_prefix_v4/10.71.2.0_24_10.71.0.1\"     //FILTER e.mt_id_tlv.mt_id == null      //RETURN CONCAT_SEPARATOR(\" -&gt; \", [p.vertices[]._key, p.vertices[].router_id, p.edges[*]._key])</p> <p>for u in unicast_prefix_v4 return u._key</p> <p>for v, e in outbound 'ls_node/2_0_0_0000.0000.0002' GRAPH ls_node return [v._key, e._key]</p> <p>//for d in ls_link filter d.mt_id_tlv.mt_id != 2 return d._key</p> <p>//for v, e in outbound shortest_path 'LSNode/2_0_0_0000.0000.0017' TO 'UnicastPrefixV4/10.71.2.0_24_10.71.0.1' LSNode_Edge filter e.mt_id != 2 return e</p> <p>//for v, e in outbound shortest_path 'ls_node/2_0_0_0000.0000.0017' TO 'unicast_prefix_v4/10.71.2.0_24_10.71.0.1' ls_node_edge filter e.mt_id != 2 return {node: v._key, link: e._key, latency: e.link_latency, asn: v.asn, local_asn: v.local_asn, remote_asn: v.remote_asn  } </p> <p>//for v, e in outbound shortest_path 'ls_node/2_0_0_0000.0000.0017' TO 'unicast_prefix_v4/10.71.2.0_24_10.71.0.1' ls_node_edge OPTIONS {weightAttribute: 'link_latency' } filter e.mt_id != 2 return { node: v._key, link: e._key, latency: e.link_latency }</p> <p>//for l in ls_link filter l.protocol_id == 7 &amp;&amp; l.peer_asn == 100000 &amp;&amp; l.remote_link_ip == \"10.73.0.1\" return { epe_sid: l.peer_node_sid.prefix_sid } </p> <p>//for l in ls_link filter l.protocol_id == 7 &amp;&amp; l.peer_asn == 100000 &amp;&amp; l.remote_link_ip == \"10.73.0.1\" return { epe_sid: l.peer_node_sid.sid }  //for l in ls_link filter l.protocol_id == 7 return l.peer_node_sid.sid//&amp;&amp;  l.remote_link_ip == \"10.73.0.1\" return l //for l in ls_link filter l._key == \"7_0_0_46489_10.0.0.43_10.73.0.0_10.0.0.73_10.73.0.1\" return l //for l in ls_link filter l.protocol_id == 7 return [l._key, l.remote_link_ip, l.peer_node_sid.sid]</p> <p>//for l in lsv4_edge return [l._key, l.link_latency] //FOR p in lsv4_edge filter p._key == \"2_0_0_0_0000.0000.0004_10.1.1.65_0000.0000.0007_10.1.1.64\" UPDATE p with { link_latency: 50 } in lsv4_edge </p> <p>//for v, e in outbound shortest_path 'ls_node/2_0_0_0000.0000.0017' TO 'unicast_prefix_v4/10.71.2.0_24_10.71.0.1' lsv4_edge filter e.mt_id != 2 return {node: v._key, link: e._key, latency: e.link_latency, asn: v.asn, local_asn: v.local_asn, remote_asn: v.remote_asn  } </p> <p>for l in l3vpn_v4_prefix filter l.base_attrs.ext_community_list like \"%100:100%\" return l//.base_attrs.ext_community_list //for l in l3vpn_v4 SEARCH  l.base_attrs.ext_community_list == \"rt=100:100\" filter l.base_attrs.local_pref != null return l//for d in l3vpn_v4 filter d.nexthop == \"10.0.0.9\" return d._key//filter l.nexthop == \"10.0.0.9\" return l</p> <p>//for l in ls_node filter l.igp_router_id == \"0000.0000.0021\" return l._id //for l in unicast_prefix_v4 filter l.prefix ==\"10.10.21.0\" return l._id</p> <p>//for l in ls_node for u in unicast_prefix_v4 filter u._key == \"10.10.3.0_24_10.0.0.3\" filter l.igp_router_id == \"0000.0000.0003\" INSERT { _from: l._id, _to: u._id, _key: \"10.10.3.0_24_10.0.0.3\" } INTO lsv4_edge</p> <p>//for l in lsv4_edge  filter l._key == \"10.10.3.0_24_10.0.0.3\" return l</p> <p>//for l in lsv4_edge filter l._key == \"10.10.3.0_24_10.0.0.3\" UPDATE l with { prefix: \"10.10.3.0\", prefix_len: 24, nexthop: \"10.0.0.3\", labels: 24031 } in lsv4_edge</p> <p>//FOR v, e, p IN 1..16 OUTBOUND 'ls_node/2_0_0_0000.0000.0019' lsv4_edge OPTIONS {uniqueVertices: \"path\", bfs: true} FILTER v._id == 'unicast_prefix_v4/10.10.21.0_24_10.0.0.21' RETURN p.edges[*].remote_igp_id//._to </p> <p>//for l in ls_srv6_sid filter l.igp_router_id == \"0000.0000.0018\" for m in ls_srv6_sid filter m.igp_router_id == \"0000.0000.0017\" for n in ls_srv6_sid filter n.igp_router_id == \"0000.0000.0016\" for o in ls_srv6_sid filter o.igp_router_id == \"0000.0000.0021\" return [l.srv6_sid, m.srv6_sid, n.srv6_sid, o.srv6_sid]</p> <p>//for l in ls_node insert l in ls_node_meta options { ignoreErrors: true }</p> <p>//for l in ls_node filter l.igp_router_id == \"0000.0000.0001\" return l</p> <p>//for l in ls_prefix filter l.prefix_attr_tlvs.ls_prefix_sid != null return l</p> <p>//for l in RT_L3VPNV4 return l</p> <p>//for l in lsv4_edge return l</p> <p>//for l in lsv4_edge filter l.protocol_id == 7 return l</p> <p>//for l in lsv4_edge filter l._to like \"%0009%\" return l._key</p> <p>//for l in lsv4_edge UPDATE l with { link_latency: 10 } in lsv4_edge</p> <p>//for l in lsv4_edge filter l._key ==\"10.71.4.0_23_10.72.0.1\" UPDATE l with { link_latency: 10 } in lsv4_edge</p> <p>//for l in lsv4_edge filter l._key == \"13400517\" UPDATE l with { link_latency: 5 } in lsv4_edge</p> <p>//for l in lsv4_edge filter l._key == \"2_0_0_0_0000.0000.0018_10.1.1.48_0000.0000.0022_10.1.1.49\" UPDATE l with { link_latency: 90 } in lsv4_edge</p> <p>//for l in lsv4_edge filter l._key == \"2_0_0_0_0000.0000.0017_10.1.1.45_0000.0000.0016_10.1.1.44\" UPDATE l with { link_latency: 90 } in lsv4_edge</p> <p>//for l in lsv4_edge filter l._key == \"2_0_0_0_0000.0000.0020_10.1.1.7_0000.0000.0009_10.1.1.6\" UPDATE l with { link_latency: 5 } in lsv4_edge</p> <p>//for l in lsv4_edge filter l._key like \"%0019%\" return { key: l._key, latency: l.link_latency }</p> <p>//for l in lsv4_edge return { key: l._key, latency: l.link_latency }</p> <p>//for v, e in outbound shortest_path 'ls_node/2_0_0_0000.0000.0019' TO 'unicast_prefix_v4/10.71.2.0_24_10.71.0.1' lsv4_edge OPTIONS {weightAttribute: 'link_latency' } filter e.mt_id != 2 return { node: v._key, link: e._key, latency: e.link_latency }</p> <p>//for l in lsv4_edge filter l._from == \"ls_prefix/2_0_0_0_0_10.0.0.7_32_0000.0000.0007\" return l //for l in l3vpn_v4_prefix_edge return l._from //for l in ls_srv6_sid_edge return l //for l in ls_prefix return l //for l in LSNode_Edge return l  //FOR d IN peer filter d.remote_bgp_id == \"10.0.0.71\" filter d.remote_ip == \"10.71.0.1\" return d //for l in ls_node_edge return l //for l in ls_node_edge return l//filter l.protocol_id == 7 return l//l.link_latency == 50 return l._key //for l in L3Underlay_Edge return l</p> <p>//FOR n in LSPrefix FILTER n.prefix == \"10.0.0.6\" RETURN n.prefix_attr_tlvs.ls_prefix_sid[*].prefix_sid</p> <p>//for p in unicast_prefix_v4 filter p._key == \"10.71.8.0_22_10.71.0.1\" return p</p> <p>//for l in epe_link return { key: l._key, latency: l.link_latency }</p> <p>//FOR p in epe_link FILTER p._key == \"7_0_0_100000_10.0.0.7_10.71.1.0_10.0.0.71_10.71.1.1\" UPDATE p with { link_latency: 60 } in epe_link</p> <p>//FOR p in epe_link UPDATE p with { link_latency: 20 } in epe_link</p> <p>//for l in UnicastPrefixV4   //  filter l.peer_ip == \"10.2.2.3\"      //return { UnicastPrefixV4: l,       //  LSLink: (for s in LSLink         //    filter s.remote_link_ip == \"10.2.2.3\"           //  return s)     //    }</p> <p>//for u in UnicastPrefixV4 for l in LSLink filter u.peer_ip == l.remote_link_ip filter u.peer_ip == \"10.71.1.1\"  return {u: u._id, l: l.remote_link_ip}</p> <p>//FOR d IN peer filter d.remote_ip == \"10.71.0.1\" FOR l in ls_link  filter d.remote_ip == l.remote_link_ip  return d</p> <p>//for l in LSNode_Edge return l//filter l._from == \"Peer/10.0.0.72_10.72.0.1\" return l</p> <p>//FOR d IN Peer filter d.remote_ip == \"10.71.1.1\" FOR l in LSLink  filter d.remote_ip == l.remote_link_ip  return { d, l }</p> <p>//for l in UnicastPrefixV4 //filter l.peer_ip == \"10.71.1.1\" return l //for s in LSLink filter l.peer_ip == s.remote_link_ip //return {l, s}</p> <p>//for l in UnicastPrefixV4 filter l._key == \"10.0.0.35_32_10.2.2.3\" return l</p> <p>//FOR d IN UnicastPrefixV4 FOR l in LSLink  filter d.peer_ip == l.remote_link_ip  filter d.peer_ip == \"10.72.0.1\" return d._key</p> <p>//for d in LSLink for l in UnicastPrefixV4 filter l.prefix == \"10.0.0.35\" filter d.remote_link_ip == l.peer_ip return d._key</p> <p>//for l in LSLink filter l.protocol_id ==7 return l </p> <p>//FOR d IN LSNode filter d.router_id == \"10.0.0.7\" filter d.domain_id == 0 return d</p> <p>//RETURN LENGTH(FOR v IN OUTBOUND SHORTEST_PATH 'LSNode/2_0_0_0000.0000.0017' TO 'UnicastPrefixV4/10.71.2.0_24_10.71.0.1' LSNode_Edge Return v)</p> <p>//FOR v, e, p IN 1..6 OUTBOUND 'ls_node/2_0_0_0000.0000.0019' lsv4_edge FILTER v._id == 'unicast_prefix_v4/10.10.21.0_24_10.0.0.21' RETURN CONCAT_SEPARATOR(\" -&gt; \", [p.vertices[]._key, p.edges[].epe_peer])</p> <p>//FOR v, e, p IN 1..10 ANY \"ls_node/2_0_0_0000.0000.0019\" lsv4_edge      //FILTER v._id == \"ls_node/2_0_0_0000.0000.0009\"    //  FILTER v._id == \"unicast_prefix_v4/10.71.2.0_24_10.71.0.1\"     //FILTER e.mt_id_tlv.mt_id == null      //RETURN CONCAT_SEPARATOR(\" -&gt; \", [p.vertices[]._key, p.vertices[].router_id, p.edges[*]._key])</p> <p>//for u in unicast_prefix_v4 return u._key //for v, e in outbound shortest_path 'ls_node/2_0_0_0000.0000.0017' TO 'unicast_prefix_v4/10.71.2.0_24_10.71.0.1' ls_node_edge return [v._key, e._key]</p> <p>//for v, e in outbound shortest_path 'ls_node/2_0_0_0000.0000.0017' TO 'unicast_prefix_v4/10.71.2.0_24_10.71.0.1' lsv4_edge return [v._key, e._key]</p> <p>//for v, e in outbound 'ls_node/2_0_0_0000.0000.0002' GRAPH ls_node return [v._key, e._key]</p> <p>//for d in ls_link filter d.mt_id_tlv.mt_id != 2 return d._key</p> <p>//FOR p in LSNode_Edge filter p._key == \"2_0_0_0_0000.0000.0004_10.1.1.21_0000.0000.0003_10.1.1.20\" UPDATE p with { link_latency: 20 } in LSNode_Edge</p> <p>//for l in LSNode_Edge return l //filter l._key == \"2_0_0_0_0000.0000.0004_10.1.1.65_0000.0000.0007_10.1.1.64\" return l</p> <p>//for v, e in outbound shortest_path 'LSNode/2_0_0_0000.0000.0017' TO 'UnicastPrefixV4/10.71.2.0_24_10.71.0.1' LSNode_Edge filter e.mt_id != 2 return e</p> <p>//for v, e in outbound shortest_path 'ls_node/2_0_0_0000.0000.0017' TO 'unicast_prefix_v4/10.71.2.0_24_10.71.0.1' ls_node_edge filter e.mt_id != 2 return {node: v._key, link: e._key, latency: e.link_latency, asn: v.asn, local_asn: v.local_asn, remote_asn: v.remote_asn  } </p> <p>//for v, e in outbound shortest_path 'ls_node/2_0_0_0000.0000.0017' TO 'unicast_prefix_v4/10.71.2.0_24_10.71.0.1' ls_node_edge OPTIONS {weightAttribute: 'link_latency' } filter e.mt_id != 2 return { node: v._key, link: e._key, latency: e.link_latency }</p> <p>//for l in ls_link filter l.protocol_id == 7 &amp;&amp; l.peer_asn == 100000 &amp;&amp; l.remote_link_ip == \"10.73.0.1\" return { epe_sid: l.peer_node_sid.prefix_sid } </p> <p>//for l in ls_link filter l.protocol_id == 7 &amp;&amp; l.peer_asn == 100000 &amp;&amp; l.remote_link_ip == \"10.73.0.1\" return { epe_sid: l.peer_node_sid.sid }  //for l in ls_link filter l.protocol_id == 7 return l.peer_node_sid.sid//&amp;&amp;  l.remote_link_ip == \"10.73.0.1\" return l //for l in ls_link filter l._key == \"7_0_0_46489_10.0.0.43_10.73.0.0_10.0.0.73_10.73.0.1\" return l //for l in ls_link filter l.protocol_id == 7 return [l._key, l.remote_link_ip, l.peer_node_sid.sid]</p> <p>//for l in lsv4_edge return [l._key, l.link_latency] //FOR p in lsv4_edge filter p._key == \"2_0_0_0_0000.0000.0004_10.1.1.65_0000.0000.0007_10.1.1.64\" UPDATE p with { link_latency: 50 } in lsv4_edge </p> <p>//for v, e in outbound shortest_path 'ls_node/2_0_0_0000.0000.0017' TO 'unicast_prefix_v4/10.71.2.0_24_10.71.0.1' lsv4_edge filter e.mt_id != 2 return {node: v._key, link: e._key, latency: e.link_latency, asn: v.asn, local_asn: v.local_asn, remote_asn: v.remote_asn  } </p> <p>//for l in l3vpn_v4_prefix return l.base_attrs.ext_community_list //for l in l3vpn_v4 SEARCH  l.base_attrs.ext_community_list == \"rt=100:100\" filter l.base_attrs.local_pref != null return l//for d in l3vpn_v4 filter d.nexthop == \"10.0.0.9\" return d._key//filter l.nexthop == \"10.0.0.9\" return l</p> <p>//for l in ls_node filter l.igp_router_id == \"0000.0000.0021\" return l._id //for l in unicast_prefix_v4 filter l.prefix ==\"10.10.21.0\" return l._id</p> <p>//for l in ls_node for u in unicast_prefix_v4 filter u._key == \"10.10.3.0_24_10.0.0.3\" filter l.igp_router_id == \"0000.0000.0003\" INSERT { _from: l._id, _to: u._id, _key: \"10.10.3.0_24_10.0.0.3\" } INTO lsv4_edge</p> <p>//for l in lsv4_edge  filter l._key == \"10.10.3.0_24_10.0.0.3\" return l</p> <p>//for l in lsv4_edge filter l._key == \"10.10.3.0_24_10.0.0.3\" UPDATE l with { prefix: \"10.10.3.0\", prefix_len: 24, nexthop: \"10.0.0.3\", labels: 24031 } in lsv4_edge</p> <p>FOR v, e, p IN 1..16 OUTBOUND 'ls_node/2_0_0_0000.0000.0019' lsv4_edge      OPTIONS {uniqueVertices: \"path\", bfs: true}     FILTER v._id == 'unicast_prefix_v4/10.10.21.0_24_10.0.0.21'      FILTER v.     RETURN p.edges[]._to //CONCAT_SEPARATOR(\" -&gt; \", [p.vertices[]._key, p.edges[*].epe_peer])</p> <p>//for l in ipv4_edge return l._to //for l in lsv4_edge return l //for l in lsv4_edge return l filter l.protocol_id == 7 filter l.local_link_ip LIKE \"%:%\" return l //for l in lsv4_edge filter l.protocol_id == 7 return l._key //for l in lsv4_edge filter l._key == \"2_0_0_0_0000.0000.0010_0.0.0.20_0000.0000.0014_0.0.0.18\" return l</p> <p>//for l in lsv4_edge filter l.prefix like \"128.107.20.0\" return { To: l._to, latency: l.link_latency }</p> <p>//for l in lsv4_edge filter l.protocol_id == 2 or l.origin_as == 11404 return l</p> <p>//for l in unicast_prefix_v4  COLLECT WITH COUNT INTO length RETURN { v4: length } //for l in unicast_prefix_v4 filter l.nexthop ==\"198.62.154.19\" COLLECT WITH COUNT INTO length RETURN {v4: length}</p> <p>//for l in lsv4_edge filter l.prefix == \"128.107.20.0\" return l//update l with { link_latency: 25 } in lsv4_edge </p> <p>//for l in lsv4_edge COLLECT WITH COUNT INTO length RETURN length</p> <p>//for l in lsv4_edge filter l.prefix LIKE \"128.107.0%\" return l</p> <p>//for l in ls_link filter l.protocol_id == 7 return l</p> <p>//for l in ls_node_edge return l</p> <p>//FOR p in lsv4_edge filter p.protocol_id == 7 UPDATE p with { link_latency: 10 } in lsv4_edge //FOR p in lsv4_edge filter p._key == \"7_0_0_65000_10.0.0.14_198.62.154.18_10.0.0.15_198.62.154.19\" UPDATE p with { link_latency: 25 } in lsv4_edge</p> <p>//for l in lsv4_edge return [l._key, l.link_latency] //FOR p in lsv4_edge filter p._key == \"2_0_0_0_0000.0000.0010_10.1.1.0_0000.0000.0014_10.1.1.1\" UPDATE p with { link_latency: 50 } in lsv4_edge </p> <p>//for v, e in outbound shortest_path \"ls_node/2_0_0_0000.0000.0008\" TO \"unicast_prefix_v4/128.107.20.0_23_198.62.154.19\" lsv4_edge OPTIONS { weightAttribute: 'link_latency' } filter e.mt_id != 2 return { node: v._key, link: e._key, latency: e.link_latency, asn: v.asn, local_asn: v.local_asn, remote_asn: v.remote_asn, adj_sid: e.peer_node_sid.sid  }</p> <p>//for v, e in outbound shortest_path \"ls_node/2_0_0_0000.0000.0008\" TO LIKE(\"unicast_prefix_v4/128.107.20.0_23_198.62.154.%\") lsv4_edge OPTIONS { weightAttribute: 'link_latency' } filter e.mt_id != 2 return { node: v._key, link: e._key, latency: e.link_latency, asn: v.asn, local_asn: v.local_asn, remote_asn: v.remote_asn, adj_sid: e.peer_node_sid.sid  }</p> <p>//for v, e in outbound shortest_path \"ls_node/2_0_0_0000.0000.0008\" TO \"unicast_prefix_v4/103.107.187.0_24_198.62.154.1\" lsv4_edge return {vertex: v._key, edge: e._key }</p> <p>//for l in ls_node filter l._key == \"10.0.0.1_198.62.154.1\" //for p in peer filter p._key == \"10.0.0.1_198.62.154.1\" return p</p> <p>//for l in ls_node filter l._key == \"2_0_0_0000.0000.0010\" return l.router_id</p> <p>//for l in ls_prefix filter l.prefix == \"10.0.0.12\" filter l.protocol_id == 2  RETURN  l.prefix_attr_tlvs.ls_prefix_sid[0].prefix_sid</p> <p>//for n in ls_node filter n._key == \"2_0_0_0000.0000.0012\" RETURN { prefix: n.router_id, srgb_start: n.ls_sr_capabilities.sr_capability_subtlv[*].sid }</p> <p>//for l in ls_node_edge return l</p> <p>//for l in lsv4_edge return l</p> <p>//for l in lsv4_edge filter l.origin_as == 11404 return l</p> <p>//for l in lsv4_edge filter l.protocol_id == 2 or l.origin_as == 11404 return l</p> <p>//for l in lsv4_edge filter l.prefix LIKE \"128.107.0%\" return l</p> <p>//for l in ls_link filter l.protocol_id == 7 return l</p> <p>//for l in ls_node_edge return l._key</p> <p>//FOR p in lsv4_edge UPDATE p with { link_latency: 10 } in lsv4_edge</p> <p>//for l in ls_node_edge return l//filter l.protocol_id == 7 return l//l.link_latency == 50 return l._key //for l in lsv4_edge return l //for l in l3vpn_v4_prefix_edge return l._from //for l in ls_srv6_sid_edge return l //for l in ls_prefix return l //for l in LSNode_Edge return l  //FOR d IN peer return d</p> <p>//for l in L3Underlay_Edge return l</p> <p>//FOR n in LSPrefix FILTER n.prefix == \"10.0.0.6\" RETURN n.prefix_attr_tlvs.ls_prefix_sid[*].prefix_sid</p> <p>//for l in LSLink filter l.protocol_id == 7 return l //for p in UnicastPrefixV4 filter p.peer_ip == \"10.71.0.1\" return p</p> <p>//for l in epe_link return { key: l._key, latency: l.link_latency }</p> <p>//FOR p in epe_link FILTER p._key == \"7_0_0_100000_10.0.0.7_10.71.1.0_10.0.0.71_10.71.1.1\" UPDATE p with { link_latency: 60 } in epe_link</p> <p>//FOR p in epe_link UPDATE p with { link_latency: 20 } in epe_link</p> <p>//for l in UnicastPrefixV4   //  filter l.peer_ip == \"10.2.2.3\"      //return { UnicastPrefixV4: l,       //  LSLink: (for s in LSLink         //    filter s.remote_link_ip == \"10.2.2.3\"           //  return s)     //    }</p> <p>//for u in UnicastPrefixV4 for l in LSLink filter u.peer_ip == l.remote_link_ip filter u.peer_ip == \"10.71.1.1\"  return {u: u._id, l: l.remote_link_ip}</p> <p>//FOR d IN peer filter d.remote_ip == \"10.71.0.1\" FOR l in ls_link  filter d.remote_ip == l.remote_link_ip  return d</p> <p>//for l in LSNode_Edge return l//filter l._from == \"Peer/10.0.0.72_10.72.0.1\" return l</p> <p>//FOR d IN Peer filter d.remote_ip == \"10.71.1.1\" FOR l in LSLink  filter d.remote_ip == l.remote_link_ip  return { d, l }</p> <p>//for l in UnicastPrefixV4 //filter l.peer_ip == \"10.71.1.1\" return l //for s in LSLink filter l.peer_ip == s.remote_link_ip //return {l, s}</p> <p>//for l in UnicastPrefixV4 filter l._key == \"10.0.0.35_32_10.2.2.3\" return l</p> <p>//FOR d IN UnicastPrefixV4 FOR l in LSLink  filter d.peer_ip == l.remote_link_ip  filter d.peer_ip == \"10.72.0.1\" return d._key</p> <p>//for d in LSLink for l in UnicastPrefixV4 filter l.prefix == \"10.0.0.35\" filter d.remote_link_ip == l.peer_ip return d._key</p> <p>//for l in LSLink filter l.protocol_id ==7 return l </p> <p>//FOR d IN LSNode filter d.router_id == \"10.0.0.7\" filter d.domain_id == 0 return d</p> <p>//RETURN LENGTH(FOR v IN OUTBOUND SHORTEST_PATH 'LSNode/2_0_0_0000.0000.0017' TO 'UnicastPrefixV4/10.71.2.0_24_10.71.0.1' LSNode_Edge Return v)</p> <p>//FOR v, e, p IN 4..5 ANY 'LSNode/2_0_0_0000.0000.0001' LSNode_Edge FILTER v._id == 'UnicastPrefixV4/72.72.1.0_24_10.71.0.1' RETURN CONCAT_SEPARATOR(\" -&gt; \", [p.vertices[]._key, p.edges[].epe_peer])</p> <p>//FOR v, e, p IN 1..3 ANY \"LSNode/2_0_0_0000.0000.0004\" LSNode_Edge  //    FILTER v._id == \"LSNode/2_0_0_0000.0000.0002\"  //    FILTER e.mt_id_tlv.mt_id == null  //    RETURN CONCAT_SEPARATOR(\" -&gt; \", [p.vertices[]._key, p.vertices[].router_id, p.edges[*]._key])</p> <p>//for u in unicast_prefix_v4 return u._key //for v, e in outbound shortest_path 'ls_node/2_0_0_0000.0000.0017' TO 'unicast_prefix_v4/10.71.2.0_24_10.71.0.1' ls_node_edge return [v._key, e._key]</p> <p>//for v, e in outbound shortest_path 'ls_node/2_0_0_0000.0000.0017' TO 'unicast_prefix_v4/10.71.2.0_24_10.71.0.1' lsv4_edge return [v._key, e._key]</p> <p>//for v, e in outbound 'ls_node/2_0_0_0000.0000.0002' GRAPH ls_node return [v._key, e._key]</p> <p>//for d in ls_link filter d.mt_id_tlv.mt_id != 2 return d._key</p> <p>//FOR p in LSNode_Edge filter p._key == \"2_0_0_0_0000.0000.0004_10.1.1.21_0000.0000.0003_10.1.1.20\" UPDATE p with { link_latency: 20 } in LSNode_Edge</p> <p>//for l in LSNode_Edge return l //filter l._key == \"2_0_0_0_0000.0000.0004_10.1.1.65_0000.0000.0007_10.1.1.64\" return l</p> <p>//for v, e in outbound shortest_path 'LSNode/2_0_0_0000.0000.0017' TO 'UnicastPrefixV4/10.71.2.0_24_10.71.0.1' LSNode_Edge filter e.mt_id != 2 return e</p> <p>//for v, e in outbound shortest_path 'ls_node/2_0_0_0000.0000.0017' TO 'unicast_prefix_v4/10.71.2.0_24_10.71.0.1' ls_node_edge filter e.mt_id != 2 return {node: v._key, link: e._key, latency: e.link_latency, asn: v.asn, local_asn: v.local_asn, remote_asn: v.remote_asn  } </p> <p>//for v, e in outbound shortest_path 'ls_node/2_0_0_0000.0000.0017' TO 'unicast_prefix_v4/10.71.2.0_24_10.71.0.1' ls_node_edge OPTIONS {weightAttribute: 'link_latency' } filter e.mt_id != 2 return { node: v._key, link: e._key, latency: e.link_latency }</p> <p>//for l in ls_link filter l.protocol_id == 7 &amp;&amp; l.peer_asn == 100000 &amp;&amp; l.remote_link_ip == \"10.73.0.1\" return { epe_sid: l.peer_node_sid.prefix_sid } </p> <p>//for l in ls_link filter l.protocol_id == 7 &amp;&amp; l.peer_asn == 100000 &amp;&amp; l.remote_link_ip == \"10.73.0.1\" return { epe_sid: l.peer_node_sid.sid }  //for l in ls_link filter l.protocol_id == 7 return l.peer_node_sid.sid//&amp;&amp;  l.remote_link_ip == \"10.73.0.1\" return l //for l in ls_link filter l._key == \"7_0_0_46489_10.0.0.43_10.73.0.0_10.0.0.73_10.73.0.1\" return l //for l in ls_link filter l.protocol_id == 7 return [l._key, l.remote_link_ip, l.peer_node_sid.sid]</p> <p>//FOR p in lsv4_edge UPDATE p with { link_latency: 10 } in lsv4_edge //for l in lsv4_edge return [l._key, l.link_latency] //FOR p in lsv4_edge filter p._key == \"2_0_0_0_0000.0000.0004_10.1.1.65_0000.0000.0007_10.1.1.64\" UPDATE p with { link_latency: 50 } in lsv4_edge </p> <p>//for v, e in outbound shortest_path 'ls_node/2_0_0_0000.0000.0017' TO 'unicast_prefix_v4/10.71.2.0_24_10.71.0.1' lsv4_edge filter e.mt_id != 2 return {node: v._key, link: e._key, latency: e.link_latency, asn: v.asn, local_asn: v.local_asn, remote_asn: v.remote_asn  } </p> <p>//for v, e in outbound shortest_path 'ls_node/2_0_0_0000.0000.0017' TO 'unicast_prefix_v4/10.71.2.0_24_10.71.0.1' lsv4_edge OPTIONS {weightAttribute: 'link_latency' } filter e.mt_id != 2 return { node: v._key, link: e._key, latency: e.link_latency }</p> <p>//for l in l3vpn_v4_prefix return l.base_attrs.ext_community_list for l in l3vpn_v4 SEARCH  l.base_attrs.ext_community_list == \"rt=100:100\" filter l.base_attrs.local_pref != null return l//for d in l3vpn_v4 filter d.nexthop == \"10.0.0.9\" return d._key//filter l.nexthop == \"10.0.0.9\" return l</p> <p>//for l in topology_nodes_Edge return l</p> <p>//for l in LSNode_Edge filter l.link_latency == 50 return l._key</p> <p>//for l in L3Underlay_Edge return l</p> <p>//FOR n in LSPrefix FILTER n.prefix == \"10.0.0.6\" RETURN n.prefix_attr_tlvs.ls_prefix_sid[*].prefix_sid</p> <p>//for l in LSLink filter l.protocol_id == 7 return l //for p in UnicastPrefixV4 filter p.peer_ip == \"10.71.0.1\" return p</p> <p>//for l in EPELink return { key: l._key, latency: l.link_latency }</p> <p>//FOR p in EPELink FILTER p._key == \"7_0_0_100000_10.0.0.7_10.71.1.0_10.0.0.71_10.71.1.1\" UPDATE p with { link_latency: 20 } in EPELink</p> <p>//for l in UnicastPrefixV4   //  filter l.peer_ip == \"10.2.2.3\"      //return { UnicastPrefixV4: l,       //  LSLink: (for s in LSLink         //    filter s.remote_link_ip == \"10.2.2.3\"           //  return s)     //    }</p> <p>//for u in UnicastPrefixV4 for l in LSLink filter u.peer_ip == l.remote_link_ip filter u.peer_ip == \"10.71.1.1\"  return {u: u._id, l: l.remote_link_ip}</p> <p>//for l in LSNode_Edge return l//filter l._from == \"Peer/10.0.0.72_10.72.0.1\" return l</p> <p>//FOR d IN Peer filter d.remote_ip == \"10.71.1.1\" FOR l in LSLink  filter d.remote_ip == l.remote_link_ip  return { d, l }</p> <p>//for l in UnicastPrefixV4 //filter l.peer_ip == \"10.71.1.1\" return l //for s in LSLink filter l.peer_ip == s.remote_link_ip //return {l, s}</p> <p>//for l in UnicastPrefixV4 filter l._key == \"10.0.0.35_32_10.2.2.3\" return l</p> <p>//FOR d IN UnicastPrefixV4 FOR l in LSLink  filter d.peer_ip == l.remote_link_ip  filter d.peer_ip == \"10.72.0.1\" return d._key</p> <p>//for d in LSLink for l in UnicastPrefixV4 filter l.prefix == \"10.0.0.35\" filter d.remote_link_ip == l.peer_ip return d._key</p> <p>//for l in LSLink filter l.protocol_id ==7 return l </p> <p>//FOR d IN LSNode filter d.router_id == \"10.0.0.7\" filter d.domain_id == 0 return d</p> <p>//RETURN LENGTH(FOR v IN OUTBOUND SHORTEST_PATH 'LSNode/2_0_0_0000.0000.0017' TO 'UnicastPrefixV4/10.71.2.0_24_10.71.0.1' LSNode_Edge Return v)</p> <p>//FOR v, e, p IN 4..5 ANY 'LSNode/2_0_0_0000.0000.0001' LSNode_Edge FILTER v._id == 'UnicastPrefixV4/72.72.1.0_24_10.71.0.1' RETURN CONCAT_SEPARATOR(\" -&gt; \", [p.vertices[]._key, p.edges[].epe_peer])</p> <p>//FOR v, e, p IN 1..3 ANY \"LSNode/2_0_0_0000.0000.0004\" LSNode_Edge  //    FILTER v._id == \"LSNode/2_0_0_0000.0000.0002\"  //    FILTER e.mt_id_tlv.mt_id == null  //    RETURN CONCAT_SEPARATOR(\" -&gt; \", [p.vertices[]._key, p.vertices[].router_id, p.edges[*]._key])</p> <p>//for v, e in outbound shortest_path 'LSNode/2_0_0_0000.0000.0017' TO 'UnicastPrefixV4/10.71.2.0_24_10.71.0.1' LSNode_Edge return [v._key, e._key]</p> <p>for v, e in outbound 'LSNode/2_0_0_0000.0000.0017' GRAPH Peer return [v._key, e._key]</p> <p>//FOR p in LSNode_Edge filter p._key == \"2_0_0_0_0000.0000.0004_10.1.1.21_0000.0000.0003_10.1.1.20\" UPDATE p with { link_latency: 20 } in LSNode_Edge</p> <p>//for l in LSNode_Edge return l //filter l._key == \"2_0_0_0_0000.0000.0004_10.1.1.65_0000.0000.0007_10.1.1.64\" return l</p> <p>//for v, e in outbound shortest_path 'LSNode/2_0_0_0000.0000.0017' TO 'UnicastPrefixV4/10.71.2.0_24_10.71.0.1' LSNode_Edge filter e.mt_id != 2 return e</p> <p>//for v, e in outbound shortest_path 'LSNode/2_0_0_0000.0000.0017' TO 'UnicastPrefixV4/10.71.2.0_24_10.71.0.1' LSNode_Edge filter e.mt_id != 2 return {node: v._key, link: e._key, latency: e.link_latency, asn: v.asn, local_asn: v.local_asn, remote_asn: v.remote_asn  } </p> <p>//for v, e in outbound shortest_path 'LSNode/2_0_0_0000.0000.0017' TO 'UnicastPrefixV4/10.71.2.0_24_10.71.0.1' LSNode_Edge OPTIONS {weightAttribute: 'link_latency' } filter e.mt_id != 2 return { node: v._key, link: e._key, latency: e.link_latency }</p> <p>//FOR p IN OUTBOUND K_SHORTEST_PATHS 'LSNode/2_0_0_0000.0000.0017' TO 'UnicastPrefixV4/10.71.2.0_24_10.71.0.1'   //LSNode_Edge     //  LIMIT 4       //RETURN {         //  nodes: p.vertices[]._key,           //latencies: p.edges[].link_latency,           //totalLatency: SUM(p.edges[*].link_latency)       //}</p> <p>//for l in topology_nodes_Edge return l</p> <p>//for l in LSNode_Edge return l</p> <p>//for l in LSNode return l</p> <p>//for l in UnicastPrefixV4 return l //filter l.router_ip == \"10.71.1.0\" return l</p> <p>//for l in L3Underlay_Edge return l</p> <p>//FOR n in LSPrefix FILTER n.prefix == \"10.0.0.6\" RETURN n.prefix_attr_tlvs.ls_prefix_sid[*].prefix_sid</p> <p>//for l in LSLink filter l.protocol_id == 7 return l //for p in UnicastPrefixV4 filter p.peer_ip == \"10.71.0.1\" return p</p> <p>//for l in EPELink return { key: l._key, latency: l.link_latency }</p> <p>//FOR p in EPELink FILTER p._key == \"7_0_0_100000_10.0.0.7_10.71.1.0_10.0.0.71_10.71.1.1\" UPDATE p with { link_latency: 20 } in EPELink</p> <p>//for l in UnicastPrefixV4   //  filter l.peer_ip == \"10.2.2.3\"      //return { UnicastPrefixV4: l,       //  LSLink: (for s in LSLink         //    filter s.remote_link_ip == \"10.2.2.3\"           //  return s)     //    }</p> <p>//for u in UnicastPrefixV4 for l in LSLink filter u.peer_ip == l.remote_link_ip filter u.peer_ip == \"10.71.1.1\"  return {u: u._id, l: l.remote_link_ip}</p> <p>//for l in LSNode_Edge return l//filter l._from == \"Peer/10.0.0.72_10.72.0.1\" return l</p> <p>//FOR d IN Peer filter d.remote_ip == \"10.71.1.1\" FOR l in LSLink  filter d.remote_ip == l.remote_link_ip  return { d, l }</p> <p>FOR d IN UnicastPrefixV4 FOR l in LSLink  filter d.peer_ip == l.remote_link_ip  filter d.peer_ip == \"10.72.0.1\" return d._key</p> <p>Queries for e in L3VPN_Topology filter e.RD == \"100:100\" return e</p> <p>for e in L3VPN_Topology filter e.RD == \"101:101\" and e.RouterID == e.SrcIP return e</p> <p>FOR L3VPN_Topology IN L3VPN_Topology   RETURN L3VPN_Topology</p> <p>FOR LSLink IN LSLink   RETURN LSLink</p> <p>RETURN LENGTH( FOR v IN OUTBOUND  SHORTEST_PATH 'LSNode/2_0_0_0000.0000.0007' TO 'LSNode/2_0_0_0000.0000.0019' LSNode_Edge  Return v )</p> <p>Clear a collection: FOR u IN EPEExternalPrefix   REMOVE u IN EPEExternalPrefix</p> <p>RETURN LENGTH( FOR v IN OUTBOUND  SHORTEST_PATH 'LSNode/172.31.101.1' TO 'LSNode/172.31.101.6' LS_Topology  Return v )</p> <p>Wow FOR prefix in LSPrefix FILTER prefix.mt_id_tlv.mt_id == 2 FILTER LENGTH(prefix.srv6_locator) SORT prefix.prefix, prefix.igp_router_id, prefix.protocol_id RETURN { proto: prefix.protocol_id, router: prefix.igp_router_id, loc: prefix.srv6_locator, attrs: prefix.prefix_attr_flags, prefix: CONCAT(prefix.prefix, \"/\", prefix.prefix_len)}</p> <p>FOR v, e, p IN 1..6 OUTBOUND \"LSNode/172.31.101.1\" LS_Topology      FILTER v._id == \"LSNode/172.31.101.6\"        RETURN { \"RouterID\": p.vertices[].RouterID, \"PrefixSID\": p.edges[].RemotePrefixSID, \"AdjSID\": p.edges[].AdjacencySID, \"Util\": p.edges[].Out_Octets }</p> <p>FOR v, e, p IN 3..3 OUTBOUND \"LSNode/172.31.101.1\" LS_Topology      FILTER v._id == \"LSNode/172.31.101.6\"        RETURN CONCAT_SEPARATOR(\" -&gt; \", p.vertices[*].RouterID)</p> <p>FOR v, e, p IN 1..6 OUTBOUND \"LSNode/172.31.101.1\" LS_Topology      FILTER v._id == \"LSNode/172.31.101.6\"        RETURN { \"RouterID\": p.vertices[].RouterID, \"PrefixSID\": p.edges[].RemotePrefixSID, \"Via\": e.ToInterfaceIP, \"Tx_bytes\": e.Out_Octets }</p> <p>FOR v, e IN OUTBOUND SHORTEST_PATH 'LSNode/172.31.101.1' TO 'LSNode/172.31.101.6' LS_Topology     OPTIONS {weightAttribute: 'PercentUtilOutbound'}     RETURN {v, e}</p> <p>FOR v, e, p IN 3..3 OUTBOUND \"LSNode/172.31.101.1\" LS_Topology      FILTER v._id == \"LSNode/172.31.101.6\"        RETURN { \"RouterID\": p.vertices[].RouterID, \"Via\": p.edges[].ToInterfaceIP }</p> <p>Steering queries:</p> <p>// Full Topology //for d in LSNode_Edge filter d._key == \"2_0_0_0_0000.0000.0018_10.1.1.2_0000.0000.0020_10.1.1.3\"  return d //for d in LSNode_Edge filter d._key == \"2_0_0_0_0000.0000.0020_10.1.1.3_0000.0000.0018_10.1.1.2\"  return d //for d in LSNode_Edge return d</p> <p>//for d in LSPrefix_Edge return d //for d in LSLink return [d._key, d.igp_router_id] //for d in Node return d //FOR d IN LSLink filter d.igp_router_id == null filter d.domain_id == 0 filter d.protocol_id == 7 return d //for d in UnicastPrefixV6 filter d.prefix == \"2001:19d0:600::\" return d //for d in UnicastPrefixV4 filter d.prefix == \"1.0.4.0\" return d //for d in UnicastPrefixV6 filter d.prefix == \"2001:420:ffff::116\" return d</p> <p>//FOR d IN LSLink filter d.protocol_id != 7 return d._key</p> <p>//for d in LSPrefix filter d.prefix == \"2001:420:ffff:1013::2\" return d</p> <p>Hop Count //RETURN LENGTH (FOR v IN ANY SHORTEST_PATH \"LSNode/2_0_0_0000.0000.0007\" TO \"LSNode/2_0_0_0000.0000.0019\" LSNode_Edge RETURN v)</p> <p>// All Paths //FOR v, e, p IN 5..5 ANY \"Node/10.2.1.0\" LSNode_Edge FILTER v._id == \"Node/198.62.154.19\" RETURN CONCAT_SEPARATOR(\" -&gt; \", [p.vertices[]._key, p.vertices[].router_id])</p> <p>//FOR v, e, p IN 4..4 ANY \"LSNode/2_0_0_0000.0000.0007\" LSNode_Edge FILTER e.mt_id_tlv != null FILTER v._id == \"LSNode/2_0_0_0000.0000.0019\" RETURN CONCAT_SEPARATOR(\" -&gt; \", [p.vertices[]._key, p.vertices[].router_id, p.edges[*]._key])</p> <p>//FOR v, e, p IN 1..3 ANY \"LSNode/2_0_0_0000.0000.0007\" LSNode_Edge      //FILTER v._id == \"LSNode/2_0_0_0000.0000.0019\"     // FILTER e.mt_id_tlv.mt_id == null      //RETURN CONCAT_SEPARATOR(\" -&gt; \", [p.vertices[]._key, p.vertices[].router_id, p.edges[*]._key])</p> <p>//FOR v, e, p IN 1..3 ANY \"LSNode/2_0_0_0000.0000.0007\" LSNode_Edge  //    FILTER v._id == \"LSNode/2_0_0_0000.0000.0019\"  //    FILTER p.edges[].mtid ALL == 2 //    RETURN { vertices: p.vertices[]._key, edges: p.edges[]._key, mtid: p.edges[].mtid }</p> <p>FOR p in LSPrefix      filter p.igp_router_id == \"0000.0000.0009\" and p.prefix_metric == null      return [p._key, p.igp_router_id, p.prefix, p.ls_prefix_sid]</p> <p>// Prefix SIDs //FOR d in LSPrefix filter d.igp_router_id == \"0000.0000.0003\" filter d.prefix == \"10.0.0.3\" return [d.ls_prefix_sid]</p> <p>//FOR d in LSPrefix filter d.igp_router_id == \"0000.0000.0018\" filter d.prefix == \"10.0.0.12\" return [d.ls_prefix_sid]</p> <p>//FOR d IN LSLink filter d.protocol_id == 7 RETURN d //FOR d in Node filter d.remote_bgp_id == \"10.0.0.15\" filter d.remote_ip == \"198.62.154.17\" return d</p> <p>//for l in LSv4_Topology return l</p> <p>//RETURN LENGTH( //FOR v IN OUTBOUND  //SHORTEST_PATH 'LSNode/0000.0000.0007' to 'LSNode/0000.0000.0020' LSv4_Topology //  Return v //  )</p> <p>//for e in LSv4_Topology filter e._key == \"0000.0000.0022_0.0.0.0_6_0000.0000.0003_0.0.0.0_11\" //return e</p> <p>//for l in LSSRv6SID return l</p> <p>for p in EPEPrefix filter p.Prefix == \"152.89.60.0\"   return p</p> <p>//for n in LSNode filter n.igp_router_id == \"0000.0000.0008\" and n.protocol_id == 2 //  return n</p> <p>//for l in LSNode_Edge return l</p> <p>//for l in LSNode return l._key</p> <p>//for l in LSNode return l</p> <p>//for l in LSSRv6SID_Edge return l</p> <p>//for l in LSPrefix_Edge return l</p> <p>//for l in LSLink filter l._key == \"7_0_0_64032_10.0.0.32_10.0.32.1_10.0.0.1_10.0.32.0\" return l</p> <p>for l in LSLink filter l.protocol_id == 7 return l</p> <p>//for l in lldp_nodes_Edge return l</p> <p>//for l in L3VPNV4_Prefix filter l._key == \"10.0.0.7:1_172.16.7.0_32\" return l</p> <p>//for l in LSPrefix filter l.prefix == \"10.0.0.7\" return l.prefix_attr_tlvs.ls_prefix_sid</p> <p>//for l in LSNode filter l.router_id == \"10.0.0.7\" return l.ls_sr_capabilities</p> <p>//for u in UnicastPrefixV4 filter u.nexthop == \"10.73.1.1\" return u._key</p> <p>//for u in UnicastPrefixV4 filter u.nexthop == \"10.73.0.1\" return {prefix: u.prefix, length: u.prefix_len }</p> <p>//for l in LSLink filter l.protocol_id == 7 &amp;&amp; l.area_id == \"46489\" return  { key: l._key, local_link_ip: l.local_link_ip, epe_sid: l.peer_node_sid.prefix_sid }</p> <p>old</p> <p>Graph traversal * lowest latency      * list the lowest cost path(s)     * is one of these paths lowest latency?          * if so, use prefix-sid         * if not, use prefix-sid of connecting node on lowest-latency path (ie, node on lowest latency path with lowest cost to destination * least utilized     * source SEA, dest NYC     * is least utilized also one of the lowest cost paths?         * yes: use prefix-sid of connecting node on least-utilized path + dest prefix-sid         * no: use prefix-sid of connecting node with lowest cost to dest and on L-U-path + dest prefix-sid </p> <p>multiple databases collections are equivalent to sql tables</p> <p>Queries See what interface = what label: for v in LinkEdgesV4 filter v.Label == \"24005\" return v Shortest latency: RETURN FLATTEN(     FOR v,e IN OUTBOUND SHORTEST_PATH 'Routers/10.1.1.0' TO 'Prefixes/10.11.0.0_24'     GRAPH \"topology\"     OPTIONS {weightAttribute: 'Latency', defaultWeight: 60}     RETURN [e.Label, v.Label] ) Shortest latency: FOR v,e IN OUTBOUND SHORTEST_PATH 'Routers/10.1.1.1' TO 'Prefixes/10.11.0.0_24' GRAPH 'topology' OPTIONS {weightAttribute: \"Latency\", defaultWeight: 1000} return e.Label Get all prefix edges: for r in PrefixEdges filter r.Latency != null and r._to == \"Prefixes/10.11.0.0_24\" return r</p> <p>Lots of example queries</p> <p>//for l in LSv4_Topology return l</p> <p>//RETURN LENGTH( //FOR v IN OUTBOUND  //SHORTEST_PATH 'LSNode/2_0__0000.0000.0004' to 'LSNode/2_0__0000.0000.0020' LSv4_Topology //  Return v //  )</p> <p>//for e in LSv4_Topology filter e._key == \"0000.0000.0022_0.0.0.0_6_0000.0000.0003_0.0.0.0_11\" //return e</p> <p>//for l in LSSRv6SID return l</p> <p>//for p in UnicastPrefix filter p.prefix == \"1.0.4.0\" //  return p</p> <p>//for n in LSNode filter n.igp_router_id == \"0000.0000.0008\" and n.protocol_id == 2 //  return n</p> <p>//for l in LSv6_Topology return l //for l in LSNode return l.srgb_start //for l in L3VPNPrefix return l //for l in LSLinkEdge return l._key //for l in LSLinkEdge filter l.local_igp_id ==\"0000.0000.0008\" return l</p> <p>//FOR l in LSNode filter l._key == \"2_0_0000.0000.0007\" return l.srgb_start </p> <p>//FOR n in LSNode RETURN n.name</p> <p>//FOR l in LSv4_Topology return l._key //FOR l in LSLinkV1 filter l._key not in @lsv4_topology_keys return l._key //FOR l in LSv4_Topology filter l._key == \"2_0_0000.0000.0022_10.1.1.4_0_0000.0000.0019_10.1.1.5_0\" RETURN { key: l._key }</p> <p>//FOR l in LSPrefixV1 filter l.igp_router_id == \"0000.0000.0022\" and l.prefix_sid != null for z in l.prefix_sid filter z.algo == 0 return {\"prefix\": l.prefix, \"length\": l.length, \"flags\": z.flags, \"sid_index\":  z.prefix_sid}</p> <p>//FOR l in LSPrefixV1 filter l.igp_router_id == \"0000.0000.0022\" return {\"prefix\": l.prefix, \"length\": l.length, \"flags\": l.flags, \"sid_index\":  l.prefix_sid}</p> <p>FOR l in LSv4_Topology filter l._key == \"2_0_0000.0000.0022_0.0.0.0_6_0000.0000.0003_0.0.0.0_11\" return {\"local_igp_id\": l.local_igp_id}</p> <p>//for l in LSv4_Topology return l</p> <p>//RETURN LENGTH( //FOR v IN OUTBOUND  //SHORTEST_PATH 'LSNodeDemo/0000.0000.0006' TO 'LSNodeDemo/0000.0000.0019' LSv4_Topology // Return v //)</p> <p>//FOR v, e, p IN 3..3 OUTBOUND \"LSNodeDemo/0000.0000.0006\" LSv4_Topology //     FILTER v._id == \"LSNodeDemo/0000.0000.0019\" //       RETURN CONCAT_SEPARATOR(\" -&gt; \", p.vertices[*].router_id)</p> <p>FOR v, e IN OUTBOUND SHORTEST_PATH 'LSNodeDemo/0000.0000.0007' TO 'LSNodeDemo/0000.0000.0019' LSv4_Topology     OPTIONS {weightAttribute: 'Percent_Util_Outbound'}     FILTER e != null     RETURN [v.router_id, e.remote_prefix_sid]</p> <p>//FOR p IN OUTBOUND K_SHORTEST_PATHS 'LSNodeDemo/0000.0000.0007' TO 'LSNodeDemo/0000.0000.0019' LSv4_Topology //LIMIT 3 //RETURN p</p> <p>//FOR v, e IN OUTBOUND  //SHORTEST_PATH 'LSNodeDemo/0000.0000.0006' TO 'LSNodeDemo/0000.0000.0019' LSv4_Topology // FILTER v.router_id != \"10.0.0.9\" // Return [v.router_id, e.remote_prefix_sid]</p> <p>//FOR v,e IN //  OUTBOUND SHORTEST_PATH \"LSNodeDemo/0000.0000.0006\" TO \"LSNodeDemo/0000.0000.0019\" GRAPH \"LSv4\" //  RETURN [v.router_id, e.remote_prefix_sid] </p> <p>//FOR path IN //  OUTBOUND K_SHORTEST_PATHS \"LSNodeDemo/0000.0000.0006\" TO \"LSNodeDemo/0000.0000.0019\" GRAPH \"LSv4\" //LIMIT 1 //RETURN path</p>"},{"location":"development/","title":"Jalapeno Development","text":"<p>This section contains helpful resources for developing &amp; contributing to Jalapeno.</p> <p>Select a section below to get started:</p> <ul> <li> Develop with a Minimal Jalapeno instance</li> <li> Learn how to Build container images</li> </ul>"},{"location":"development/contributing/","title":"Contribute to Jalapeno","text":"<p>Abstract</p> <p>Content coming soon...</p>"},{"location":"development/images/","title":"Build &amp; Deploy Images","text":"<p>This page contains the steps to build Jalapeno docker images &amp; publish to a docker respository.</p>"},{"location":"development/images/#building-an-image","title":"Building an image","text":"<p>To build an image, locate the <code>Makefile</code> in the directory of whichever Jalapeno service you're working on. This <code>Makefile</code> defines the image ID and tag number of the Jalapeno service being deployed.</p> <p>For example:</p> <pre><code>REPO=iejalapeno\nIMAGE=api\nTAG=0.0.1.2\n</code></pre> <p>After you have finished developing your service code, open the <code>Makefile</code> and change the tag number to avoid overwriting pre-existing functional code.</p> <p>Then, execute in your terminal:</p> <pre><code>make container\n</code></pre> <p>If there are no errors, the image will build.</p>"},{"location":"development/images/#pushing-an-image","title":"Pushing an image","text":"<p>To push a Jalapeno docker image to a docker repository:</p> <ol> <li> <p>Log into docker (assuming you have permissions to the dockerhub repository)</p> <pre><code>docker login\n</code></pre> </li> <li> <p>List images you've built locally:</p> <pre><code>docker images\n</code></pre> </li> <li> <p>Locate the image you want to push and use the following command, replacing the repository name &amp; tag as needed:</p> <pre><code>docker push [REPOSTIORY NAME]:[TAG]\ndocker push iejalapeno/api:0.0.3 # sample command\n</code></pre> </li> </ol>"},{"location":"development/images/#using-an-image","title":"Using an image","text":"<p>Once you have built and pushed a new image to the repository, you can pull it into the Jalapeno environment:</p> <pre><code>docker pull [REPOSTIORY NAME]:[TAG]\ndocker pull iejalapeno/api:0.0.3 # sample command\n</code></pre> <p>To use the image, find the core YAML file that defines how the Jalapeno service is deployed. For example, in the API service, the image is loaded in <code>api.yaml</code>. In this YAML file, there should be a pre-existing image listed in the code. This should look similar to the output below:</p> <pre><code>containers:\n      - name: api\n        image: iejalapeno/api:0.0.1.2@sha256:e35d9ad6a3a10ad4d39c3310c29b460afbf43ed9efeaf1fd5041881dafb24357\n</code></pre> <p>In this YAML file, update the image tag and SHA256 key (which should be returned any time you pull or push the image).</p> <p>If the service IS NOT already runnning with a prior image, run the following to deploy the Jalapeno service with the new image:</p> <pre><code>oc apply -f ./api.yaml #(1)!\n</code></pre> <ol> <li>Assumes you have the OpenShift CLI tool \"oc\" installed, this is currently installed on the CentosKVM on all Jalapeno servers</li> </ol> <p>If you've updated an image and wish to deploy it in an existing Jalapeno cluster:</p> <ol> <li>Go to your microk8s web UI and navigate to deployments.</li> <li>Click on the image you wish to update, and then click on the edit (pencil) icon</li> <li>Update the YAML file with the new image tag and SHA256 key.</li> <li>Microk8s will automatically delete the old Pod and bring up a new one using the updated image.</li> </ol> <p>And just like that, you've successfully built, pushed, and used your own Jalapeno image! \ud83c\udf89</p>"},{"location":"development/minimal/","title":"Minimal Instance","text":"<p>For local development &amp; testing, you may not have resources for a full installation of Jalapeno. In that case, you might want to spin up a minimal set of Jalapeno's services.</p> <p>With the minimal deployment:</p> <ul> <li>No collectors are deployed</li> <li>No Kafka services</li> <li>Only the Topology processor is included</li> </ul> <p>To install the minimal version:</p> <ol> <li> <p>Clone the Jalapeno repo and <code>cd</code> into the folder:</p> <pre><code>git clone https://github.com/cisco-open/jalapeno.git &amp;&amp; cd jalapeno\n</code></pre> </li> <li> <p>Use the <code>deploy_minimal_jalapeno.sh</code> script to start the Jalapeno services:</p> <pre><code>./deploy_minimal_jalapeno.sh [path_to_kubectl]\n</code></pre> </li> </ol> <p>To destroy the minimal deployment:</p> <ol> <li> <p>Use the <code>destroy_minimal_jalapeno.sh</code> script:</p> <pre><code>destroy_jalapeno.sh kubectl\n</code></pre> </li> </ol>"},{"location":"device-config/","title":"Device Configuration","text":"<p>This section will contain notes and configuration snippets for getting devices to report information to Jalapeno.</p>"},{"location":"device-config/#general-notes","title":"General Notes","text":"<p>Jalapeno uses the following default ports:</p> <ul> <li>TCP/32400 for gRPC-based MDT</li> <li>TCP/30511 for BMP</li> </ul> <p>Generally it is recommended to setup MDT on all routers, but only enable BMP only on route reflectors &amp; any routers with external peering sessions.</p>"},{"location":"device-config/#device-specific-config","title":"Device Specific Config","text":"<p>Currently, we have documentation for the following platforms:</p> <ul> <li> Cisco IOS-XR</li> </ul> <p>Missing a platform? Feel free to submit an issue to request it!</p> <p></p>"},{"location":"device-config/xr-config/","title":"IOS-XR","text":"<p>This section contains notes and example IOS-XR router configs and network design.</p>"},{"location":"device-config/xr-config/#segment-routing-and-srv6","title":"Segment Routing and SRv6","text":"<ol> <li> <p>Base SR/SRv6 configuration</p> <pre><code>segment-routing\n global-block 16000 23999   \n srv6\n  encapsulation\n  source-address fc00:0000:6::1\n !\n locators\n  locator MAIN\n    micro-segment behavior unode psp-usd\n    prefix fc00:0000:6::/48\n!\n</code></pre> </li> <li> <p>Enable SR/SRv6 in ISIS</p> <pre><code>router isis 100\n is-type level-2-only\n net 49.0901.0000.0000.0006.00\n address-family ipv4 unicast\n  metric-style wide\n  advertise link attributes\n  mpls traffic-eng level-2-only\n  mpls traffic-eng router-id Loopback0\n  maximum-paths 32\n  segment-routing mpls\n  segment-routing srv6\n   locator MAIN\n  !\n !\n interface Loopback0\n  passive\n  address-family ipv4 unicast\n   prefix-sid absolute 16006 #(1)!\n  !\n</code></pre> <ol> <li>This can also be an SRGB index value</li> </ol> </li> <li> <p>Enable distribution of SR Prefix-SID information in BGP-LU (Optional, but very valuable for multi-domain networks)</p> <pre><code>route-policy SID($SID)\n  set label-index $SID\nend-policy\n!\nrouter bgp 100000\n!\n address-family ipv4 unicast\n  network 10.0.0.1/32 route-policy SID(1) #(1)! \n  allocate-label all\n !\n neighbor 10.1.1.0   #(2)!\n  remote-as 65000\n  !       \n  address-family ipv4 labeled-unicast\n   route-policy pass in\n   route-policy pass out\n!\n</code></pre> <ol> <li>BGP Prefix-SID index 1, results in SR label value 100001 based on our SRGB</li> <li>ASBR</li> </ol> </li> </ol>"},{"location":"device-config/xr-config/#setting-up-bmp-and-bgp-ls","title":"Setting up BMP and BGP-LS","text":""},{"location":"device-config/xr-config/#diagram","title":"Diagram","text":""},{"location":"device-config/xr-config/#bgp-ls","title":"BGP-LS","text":"<ol> <li> <p>Setup route-reflectors to receive LS messages from clients, but to not pass LS messages back out:</p> <pre><code>BGP-LS specific information shown:\n\nrouter bgp 100000\n !\n address-family link-state link-state\n !\n neighbor &lt;neighbor IP&gt;\n  address-family link-state link-state\n   route-policy pass in\n   route-policy drop out\n</code></pre> </li> </ol> <p>Note</p> <p>The RR's technically only need a copy of BGP-LS/LSDB from one router.</p> <ol> <li> <p>Configure one or more RR clients to pass LS messages to the RR's.  </p> <p>BGP-LS specific information shown:</p> <pre><code>router isis 100\n distribute link-state level 2 #(1)!\n !\n address-family ipv4 unicast\n  advertise link attributes #(2)!\n !\nrouter bgp 100000\n !\n address-family link-state link-state\n !\n neighbor 192.0.2.50 #(3)!\n  !\n  address-family link-state link-state\n   route-policy drop in\n   route-policy pass out\n</code></pre> <ol> <li>Distribute my LSDB into local BGP-LS</li> <li>All ISIS nodes should have this line as it adds their SR TLVs into the domain's LSDB</li> <li>Route Reflector</li> </ol> </li> <li> <p>Add Egress Peer Engineering data to BGP-LS feed (Optional)</p> <p>On an ASBR node, or Internet peering node add the following (it is assumed v4/v6 AFIs are already enabled):</p> <pre><code>router bgp 100000\n !\n address-family link-state link-state\n !\n neighbor 192.0.2.50 #(1)!\n  !       \n  address-family link-state link-state\n   route-policy pass out\n   !\n  !\n neighbor 203.0.113.50 #(2)!\n  remote-as 64496 \n  egress-engineering\n !\n</code></pre> <ol> <li>Route Reflector</li> <li>External Peer</li> </ol> </li> </ol>"},{"location":"device-config/xr-config/#bmp-bgp-monitoring-protocol","title":"BMP (BGP Monitoring Protocol)","text":"<p>We collect topology data with BMP as it provides data from multiple AFI/SAFI combinations (not just BGP-LS) While we anticipate most operators to run a BGP-free core, we'll generally want to collect BMP messages from route-reflectors and all BGP speakers with external facing peering sessions (ASBRs, Peering, etc.):</p> <ol> <li> <p>Configure BMP Server on RR's and on ASBRs and peering routers:</p> <pre><code>bmp server 1\n host 10.0.250.2 port 30511\n description Jalapeno GoBMP \n update-source Loopback0 #(1)!\n flapping-delay 60\n initial-delay 5\n stats-reporting-period 60\n initial-refresh delay 30 spread 2\n</code></pre> <ol> <li>Alternatively, <code>update-source MgmtEth0/RP0/CPU0/0</code></li> </ol> </li> <li> <p>Configure export of BMP data for BGP messages/advertisements from specific peers:</p> <ol> <li> <p>Route reflector</p> <pre><code>router bgp 100000\nneighbor &lt;neighbor IP&gt; #(1)!\nbmp-activate server 1\n</code></pre> <ol> <li>Assuming clients are VPNv4/v6 PE's, this also captures VPNv4/6 messages</li> </ol> </li> <li> <p>ASBR/Peering</p> <pre><code> router bgp 100000\n !\n neighbor 192.0.2.100\n  remote-as 64500\n  description External Peer 72\n  egress-engineering\n  bmp-activate server 1\n</code></pre> </li> </ol> </li> </ol>"},{"location":"device-config/xr-config/#streaming-telemetry-model-driven-telemetry","title":"Streaming Telemetry (Model Driven Telemetry)","text":"<p>The following configuration snippet provides a reference for sending streaming telemetry to Jalapeno.</p> <pre><code>telemetry model-driven\ndestination-group jalapeno\n    vrf &lt;name&gt; // optional \n    address-family ipv4 192.0.2.10 port 32400\n    encoding self-describing-gpb\n    protocol grpc no-tls\n    !\n!\nsensor-group cisco_models \n    sensor-path Cisco-IOS-XR-pfi-im-cmd-oper:interfaces/interface-xr/interface #(1)!\n    sensor-path Cisco-IOS-XR-infra-tc-oper:traffic-collector/afs/af/counters/prefixes/prefix #(2)!\n    sensor-path Cisco-IOS-XR-fib-common-oper:mpls-forwarding/nodes/node/label-fib/forwarding-details/forwarding-detail #(3)!\n!\nsensor-group openconfig_interfaces\n    sensor-path openconfig-interfaces:interfaces/interface #(4)!\n!\nsubscription base_metrics\n    sensor-group-id cisco_models sample-interval 10000\n    sensor-group-id openconfig_interfaces sample-interval 10000\n    destination-id jalapeno\n    source-interface MgmtEth0/RP0/CPU0/0\n!\n</code></pre> <ol> <li>Interface statistics</li> <li>SR traffic collector statistics</li> <li>Per-MPLS label forwarding statistics</li> <li>Openconfig interface statistics</li> </ol>"},{"location":"install/gettingstarted/","title":"Getting Started","text":"<p>This section contains instructions for setting up Jalapeno. This application is built using microservices on top of a Kubernetes (K8s) cluster.</p> <p></p> <ul> <li> Start with the Prerequisites</li> <li> No Kubernetes? Set up a cluster</li> <li> Jump right into Installing Jalapeno</li> <li> Need more background? Learn about the project</li> </ul>"},{"location":"install/jalapeno/","title":"Jalapeno Installation","text":"<p>Warning</p> <p>A Kubernetes installation is required to continue. If you don't have a running environment, please follow the steps here to get set up.</p>"},{"location":"install/jalapeno/#installing-jalapeno","title":"Installing Jalapeno","text":"Tip <p>The Jalapeno installation script by default will pull a telemetry stack consisting of Telegraf, Influx, and Kafka images (the TIK stack). If you would like to  integrate Jalapeno's BMP/Topology/GraphDB elements with an existing telemetry stack simply comment out the TIK stack elements in the shell script.</p> <ol> <li> <p>Clone the Jalapeno repo and <code>cd</code> into the folder:</p> <pre><code>git clone https://github.com/cisco-open/jalapeno.git &amp;&amp; cd jalapeno/install\n</code></pre> </li> <li> <p>Use the <code>deploy_jalapeno.sh</code> script. This will start the collectors, the Jalapeno infra images, and the topology and linkstate-edge processors.</p> <pre><code>./deploy_jalapeno.sh [path_to_kubectl]\n</code></pre> <p>Tip</p> <p>If you're using a nonstandard kubectl, you need to pass the appropriate command to this script.</p> <p>For example, with microk8s: <code>./deploy_jalapeno.sh microk8s.kubectl</code></p> </li> </ol>"},{"location":"install/jalapeno/#validation","title":"Validation","text":"<p>Validate that all containers are started &amp; running. Using: <code>kubectl get all --all-namespaces</code> or on a per-namespace basis:</p> <pre><code>kubectl get all -n jalapeno\n</code></pre> <p>Expected Output for <code>jalapeno</code> Namespace:</p> <pre><code>NAME                                              READY   STATUS    RESTARTS   AGE\npod/arangodb-0                                    1/1     Running   0          9d\npod/grafana-deployment-579c5f75bb-7g7bk           1/1     Running   0          9d\npod/influxdb-0                                    1/1     Running   0          9d\npod/kafka-0                                       1/1     Running   0          9d\npod/linkstate-edge-66fb9b8fb7-skmsc               1/1     Running   0          6d22h\npod/telegraf-egress-deployment-55cbff896c-vf26q   1/1     Running   3          9d\npod/topology-6fdd6ccc8b-6mcpc                     1/1     Running   0          8d\npod/zookeeper-0                                   1/1     Running   0          9d\n\nNAME                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE\nservice/arango-np     NodePort    10.152.183.143   &lt;none&gt;        8529:30852/TCP               9d\nservice/arangodb      ClusterIP   10.152.183.142   &lt;none&gt;        8529/TCP                     9d\nservice/broker        ClusterIP   10.152.183.247   &lt;none&gt;        9092/TCP                     9d\nservice/grafana       ClusterIP   10.152.183.62    &lt;none&gt;        3000/TCP                     9d\nservice/grafana-np    NodePort    10.152.183.124   &lt;none&gt;        3000:30300/TCP               9d\nservice/influxdb      ClusterIP   10.152.183.197   &lt;none&gt;        8086/TCP                     9d\nservice/influxdb-np   NodePort    10.152.183.68    &lt;none&gt;        8086:30308/TCP               9d\nservice/kafka         NodePort    10.152.183.160   &lt;none&gt;        9094:30092/TCP               9d\nservice/zookeeper     ClusterIP   10.152.183.36    &lt;none&gt;        2888/TCP,3888/TCP,2181/TCP   9d\n\nNAME                                         READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/grafana-deployment           1/1     1            1           9d\ndeployment.apps/linkstate-edge               1/1     1            1           6d22h\ndeployment.apps/telegraf-egress-deployment   1/1     1            1           9d\ndeployment.apps/topology                     1/1     1            1           8d\n\nNAME                                                    DESIRED   CURRENT   READY   AGE\nreplicaset.apps/grafana-deployment-579c5f75bb           1         1         1       9d\nreplicaset.apps/linkstate-edge-66fb9b8fb7               1         1         1       6d22h\nreplicaset.apps/telegraf-egress-deployment-55cbff896c   1         1         1       9d\nreplicaset.apps/topology-6fdd6ccc8b                     1         1         1       8d\n\nNAME                         READY   AGE\nstatefulset.apps/arangodb    1/1     9d\nstatefulset.apps/influxdb    1/1     9d\nstatefulset.apps/kafka       1/1     9d\nstatefulset.apps/zookeeper   1/1     9d\n</code></pre>"},{"location":"install/jalapeno/#device-config","title":"Device Config","text":"<p>Configure routers in the network to stream telemetry and BMP data to the Jalapeno cluster.</p> <p>Instructions can be found under the Device Config section.</p>"},{"location":"install/jalapeno/#destroying-jalapeno","title":"Destroying Jalapeno","text":"<p>Jalapeno can also be destroyed using the script.</p> <ol> <li>Use the <code>destroy_jalapeno.sh</code> script. This will remove the <code>jalapeno</code> namespace and all associated services/pods/deployments/etc. This will also remove all the persistent volumes associated with Kafka and Arangodb.</li> </ol> <pre><code>./destroy_jalapeno.sh kubectl\n</code></pre>"},{"location":"install/kubernetes/","title":"Kubernetes","text":"<p>Info</p> <p>If you already have a Kubernetes environment deployed you may skip this document and move on to the Jalapeno Install</p> <p>The following instructions present two options for installing Kubernetes:</p> <ol> <li>Kubernetes/Kubeadm</li> <li>Microk8s</li> </ol>"},{"location":"install/kubernetes/#kubernetes-install","title":"Kubernetes Install","text":"<p>Setting up a K8s cluster is outside the scope of this documentation. Instead, begin with the following guides:</p> <ol> <li> <p>Install Kubeadm</p> </li> <li> <p>Create a cluster</p> </li> </ol> Note <p>Ubuntu system pre-flight checks may error out with:</p> <p><code>[WARNING IsDockerSystemdCheck]: detected \"cgroupfs\" as the Docker cgroup driver. The recommended driver is \"systemd\". Please follow the guide at https://kubernetes.io/docs/setup/cri/</code></p> <p>To change Docker driver to systemd add this line to <code>/etc/docker/daemon.json</code>: </p> <pre><code>{\n    \"exec-opts\": [\"native.cgroupdriver=systemd\"]\n}\n</code></pre> <p>Then restart docker with <code>sudo systemctl restart docker.service</code></p>"},{"location":"install/kubernetes/#install-cilium-cni-optional","title":"Install Cilium CNI (Optional)","text":"<p>If desired, install the Cilium CNI using the instructions available here.</p>"},{"location":"install/kubernetes/#k8s-validation","title":"K8s Validation","text":"<p>Once installation is complete the cluster should look similar to the output below:</p> <pre><code>$ kubectl get all --all-namespaces\n\nNAMESPACE             NAME                                               READY   STATUS    RESTARTS      AGE\njalapeno              pod/gobmp-54cc8cf9b9-4kfrj                         1/1     Running   1 (10d ago)   20d\njalapeno              pod/telegraf-ingress-deployment-77f868dd79-8fnz8   1/1     Running   2 (10d ago)   20d\njalapeno              pod/arangodb-0                                     1/1     Running   1 (10d ago)   20d\njalapeno              pod/grafana-deployment-58986bc44b-gpq7n            1/1     Running   1 (10d ago)   20d\njalapeno              pod/influxdb-0                                     1/1     Running   1 (10d ago)   20d\njalapeno              pod/kafka-0                                        1/1     Running   2 (10d ago)   20d\njalapeno              pod/lslinknode-edge-744bd66695-pzhk2               1/1     Running   6 (10d ago)   20d\njalapeno              pod/telegraf-egress-deployment-84448c9879-dw8mc    1/1     Running   5 (10d ago)   20d\njalapeno              pod/topology-665c776f84-l8896                      1/1     Running   0             9d\njalapeno              pod/zookeeper-0                                    1/1     Running   1 (10d ago)   20d\nkube-system           pod/cilium-envoy-kqjmq                             1/1     Running   1 (10d ago)   20d\nkube-system           pod/cilium-operator-54c7465577-fvcms               1/1     Running   1 (10d ago)   20d\nkube-system           pod/cilium-trrw8                                   1/1     Running   1 (10d ago)   20d\nkube-system           pod/coredns-7c65d6cfc9-fdzc4                       1/1     Running   1 (10d ago)   20d\nkube-system           pod/coredns-7c65d6cfc9-v4mv9                       1/1     Running   1 (10d ago)   20d\nkube-system           pod/etcd-jalapeno-host                             1/1     Running   1 (10d ago)   20d\nkube-system           pod/kube-apiserver-jalapeno-host                   1/1     Running   1 (10d ago)   20d\nkube-system           pod/kube-controller-manager-jalapeno-host          1/1     Running   1 (10d ago)   20d\nkube-system           pod/kube-proxy-dts8w                               1/1     Running   1 (10d ago)   20d\nkube-system           pod/kube-scheduler-jalapeno-host                   1/1     Running   1 (10d ago)   20d\n\nNAMESPACE             NAME                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                          AGE\ndefault               service/kubernetes            ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP                          20d\njalapeno              service/gobmp                 NodePort    10.96.245.167    &lt;none&gt;        5000:30511/TCP,56767:30767/TCP   20d\njalapeno              service/telegraf-ingress-np   NodePort    10.97.247.231    &lt;none&gt;        57400:32400/TCP                  20d\njalapeno              service/arango-np             NodePort    10.111.203.73    &lt;none&gt;        8529:30852/TCP                   20d\njalapeno              service/arangodb              ClusterIP   10.99.179.251    &lt;none&gt;        8529/TCP                         20d\njalapeno              service/broker                ClusterIP   10.103.212.223   &lt;none&gt;        9092/TCP                         20d\njalapeno              service/grafana               ClusterIP   10.99.46.64      &lt;none&gt;        3000/TCP                         20d\njalapeno              service/grafana-np            NodePort    10.104.190.91    &lt;none&gt;        3000:30300/TCP                   20d\njalapeno              service/influxdb              ClusterIP   10.111.183.55    &lt;none&gt;        8086/TCP                         20d\njalapeno              service/influxdb-np           NodePort    10.97.60.195     &lt;none&gt;        8086:30308/TCP                   20d\njalapeno              service/kafka                 NodePort    10.97.226.142    &lt;none&gt;        9094:30092/TCP                   20d\njalapeno              service/zookeeper             ClusterIP   10.109.47.153    &lt;none&gt;        2888/TCP,3888/TCP,2181/TCP       20d\nkube-system           service/cilium-envoy          ClusterIP   None             &lt;none&gt;        9964/TCP                         20d\nkube-system           service/hubble-peer           ClusterIP   10.110.213.58    &lt;none&gt;        443/TCP                          20d\nkube-system           service/kube-dns              ClusterIP   10.96.0.10       &lt;none&gt;        53/UDP,53/TCP,9153/TCP           20d\n\nNAMESPACE     NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\nkube-system   daemonset.apps/cilium         1         1         1       1            1           kubernetes.io/os=linux   20d\nkube-system   daemonset.apps/cilium-envoy   1         1         1       1            1           kubernetes.io/os=linux   20d\nkube-system   daemonset.apps/kube-proxy     1         1         1       1            1           kubernetes.io/os=linux   20d\n\nNAMESPACE             NAME                                          READY   UP-TO-DATE   AVAILABLE   AGE\njalapeno              deployment.apps/gobmp                         1/1     1            1           20d\njalapeno              deployment.apps/telegraf-ingress-deployment   1/1     1            1           20d\njalapeno              deployment.apps/grafana-deployment            1/1     1            1           20d\njalapeno              deployment.apps/lslinknode-edge               1/1     1            1           20d\njalapeno              deployment.apps/telegraf-egress-deployment    1/1     1            1           20d\njalapeno              deployment.apps/topology                      1/1     1            1           20d\nkube-system           deployment.apps/cilium-operator               1/1     1            1           20d\nkube-system           deployment.apps/coredns                       2/2     2            2           20d\n\nNAMESPACE             NAME                                                     DESIRED   CURRENT   READY   AGE\njalapeno              replicaset.apps/gobmp-54cc8cf9b9                         1         1         1       20d\njalapeno              replicaset.apps/telegraf-ingress-deployment-77f868dd79   1         1         1       20d\njalapeno              replicaset.apps/grafana-deployment-58986bc44b            1         1         1       20d\njalapeno              replicaset.apps/lslinknode-edge-744bd66695               1         1         1       20d\njalapeno              replicaset.apps/telegraf-egress-deployment-84448c9879    1         1         1       20d\njalapeno              replicaset.apps/topology-665c776f84                      1         1         1       20d\nkube-system           replicaset.apps/cilium-operator-54c7465577               1         1         1       20d\nkube-system           replicaset.apps/coredns-7c65d6cfc9                       2         2         2       20d\n\nNAMESPACE   NAME                         READY   AGE\njalapeno    statefulset.apps/arangodb    1/1     20d\njalapeno    statefulset.apps/influxdb    1/1     20d\njalapeno    statefulset.apps/kafka       1/1     20d\njalapeno    statefulset.apps/zookeeper   1/1     20d\n</code></pre> <p>If everything looks good, move onto Installing Jalepeno.</p>"},{"location":"install/kubernetes/#microk8s-install","title":"MicroK8s Install","text":"<p>The following instructions may be used to install a single-node Microk8s cluster.  </p>"},{"location":"install/kubernetes/#installing-microk8s-on-ubuntu","title":"Installing MicroK8s on Ubuntu","text":"<p>The following documentation is based on this guide.</p> <ol> <li> <p>Ensure that <code>snap</code> is installed using <code>snap version</code>.</p> <ul> <li>If necessary, install snap with <code>sudo apt install snapd</code> or see the instructions here.</li> </ul> </li> <li> <p>Install MicroK8s using <code>sudo snap install microk8s --classic</code></p> <ul> <li>Optionally specify a specific channel using the flag <code>--channel=1.17/stable</code></li> </ul> </li> <li> <p>Add user into microk8s group with <code>sudo usermod -a -G microk8s $USER</code></p> <ul> <li>You will need to reinstantiate shell for this to take effect</li> </ul> </li> <li> <p>Configure firewall to allow pod to pod communication (Note: If ufw is not installed use <code>sudo apt install ufw</code>):</p> <pre><code>sudo apt install ufw\nsudo ufw allow in on cni0 &amp;&amp; sudo ufw allow out on cni0\nsudo ufw default allow routed\n</code></pre> </li> <li> <p>If your cluster is behind a proxy, add the following to <code>/var/snap/microk8s/current/args/containerd-env</code>:</p> <pre><code>HTTPS_PROXY=http://&lt;your_proxy&gt;\nNO_PROXY=10.0.0.0/8\n</code></pre> <p>Then restart containerd with <code>sudo systemctl restart snap.microk8s.daemon-containerd.service</code></p> </li> <li> <p>Enable dashboard helm and dns: <code>microk8s.enable dashboard dns</code></p> </li> <li> <p>Check if services are up: <code>microk8s.kubectl get all --all-namespaces</code>.</p> <p>The output should look like below. Take note of the columns <code>READY</code> and <code>STATUS</code>.</p> <pre><code>$ microk8s.kubectl get all --all-namespaces\nNAMESPACE     NAME                                                  READY   STATUS    RESTARTS   AGE\nkube-system   pod/coredns-9b8997588-qxxpf                           1/1     Running   0          2d4h\nkube-system   pod/dashboard-metrics-scraper-687667bb6c-k85m7        1/1     Running   0          2d4h\nkube-system   pod/heapster-v1.5.2-5c58f64f8b-c8h4x                  4/4     Running   0          2d4h\nkube-system   pod/kubernetes-dashboard-5c848cc544-xrtgw             1/1     Running   0          2d4h\nkube-system   pod/monitoring-influxdb-grafana-v4-6d599df6bf-2c5g5   2/2     Running   0          2d4h\n\nNAMESPACE     NAME                                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE\ndefault       service/kubernetes                  ClusterIP   10.152.183.1     &lt;none&gt;        443/TCP                  3d3h\nkube-system   service/dashboard-metrics-scraper   ClusterIP   10.152.183.131   &lt;none&gt;        8000/TCP                 2d4h\nkube-system   service/heapster                    ClusterIP   10.152.183.170   &lt;none&gt;        80/TCP                   2d4h\nkube-system   service/kube-dns                    ClusterIP   10.152.183.10    &lt;none&gt;        53/UDP,53/TCP,9153/TCP   2d4h\nkube-system   service/kubernetes-dashboard        ClusterIP   10.152.183.242   &lt;none&gt;        443/TCP                  2d4h\nkube-system   service/monitoring-grafana          ClusterIP   10.152.183.133   &lt;none&gt;        80/TCP                   2d4h\nkube-system   service/monitoring-influxdb         ClusterIP   10.152.183.87    &lt;none&gt;        8083/TCP,8086/TCP        2d4h\n\nNAMESPACE     NAME                                             READY   UP-TO-DATE   AVAILABLE   AGE\nkube-system   deployment.apps/coredns                          1/1     1            1           2d4h\nkube-system   deployment.apps/dashboard-metrics-scraper        1/1     1            1           2d4h\nkube-system   deployment.apps/heapster-v1.5.2                  1/1     1            1           2d4h\nkube-system   deployment.apps/kubernetes-dashboard             1/1     1            1           2d4h\nkube-system   deployment.apps/monitoring-influxdb-grafana-v4   1/1     1            1           2d4h\n\nNAMESPACE     NAME                                                        DESIRED   CURRENT   READY   AGE\nkube-system   replicaset.apps/coredns-9b8997588                           1         1         1       2d4h\nkube-system   replicaset.apps/dashboard-metrics-scraper-687667bb6c        1         1         1       2d4h\nkube-system   replicaset.apps/heapster-v1.5.2-5c58f64f8b                  1         1         1       2d4h\nkube-system   replicaset.apps/kubernetes-dashboard-5c848cc544             1         1         1       2d4h\nkube-system   replicaset.apps/monitoring-influxdb-grafana-v4-6d599df6bf   1         1         1       2d4h\n</code></pre> </li> <li> <p>Edit dashboard yaml config to change ClusterIP to NodePort:</p> <pre><code>microk8s.kubectl -n kube-system edit service kubernetes-dashboard\n</code></pre> <p>Changes should be similar to the following:</p> <pre><code>spec:\n  clusterIP: 10.152.183.93\n  externalTrafficPolicy: Cluster\n  ports:\n  - nodePort: 31444\n    port: 443\n    protocol: TCP\n    targetPort: 8443\n  selector:\n    k8s-app: kubernetes-dashboard\n  sessionAffinity: None\n  type: NodePort\n</code></pre> </li> <li> <p>Enable skip for login token (only way over http proxy)</p> <ol> <li> <p><code>microk8s.kubectl -n kube-system edit deploy kubernetes-dashboard -o yaml</code></p> </li> <li> <p>Add the <code>-enable-skip-login</code> flag to deployment's specs</p> <pre><code>spec:\n  progressDeadlineSeconds: 600\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      k8s-app: kubernetes-dashboard\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        k8s-app: kubernetes-dashboard\n    spec:\n      containers:\n      - args:\n        - --auto-generate-certificates\n        - --namespace=kube-system\n        - --enable-skip-login\n</code></pre> </li> </ol> </li> <li> <p>Enable Kubernetes proxy in the background to access dashboard from your browser:</p> <pre><code>microk8s.kubectl proxy --accept-hosts=.* --address=0.0.0.0 &amp;\n</code></pre> </li> <li> <p>Access Kubernetes Dashboard</p> <p>To get Dashboard port number:</p> <pre><code>microk8s kubectl -n kube-system get service kubernetes-dashboard\n</code></pre> <p>Then attempt to reach the web UI at: <code>https://&lt;server_ip&gt;:&lt;port_number&gt;/pod?namespace=_all</code></p> </li> <li> <p>(Recommended) Enable 'kubectl' without needing to type 'microk8s kubectl ' <pre><code>sudo chown -f -R $USER ~/.kube\nsudo snap install kubectl --classic\nmicrok8s kubectl config view --raw &gt; $HOME/.kube/config\n</code></pre> <p>You may now proceed to Install Jalapeno on your Microk8s cluster</p>"},{"location":"install/kubernetes/#removing-microk8s","title":"Removing Microk8s","text":"<p>Simply shutdown the cluster then remove Microk8s with snap:</p> <pre><code>microk8s stop\nsudo snap remove microk8s\n# verify the microk8s directory has been removed:\nls /var/snap \n</code></pre>"},{"location":"install/prerequisites/","title":"Prerequisites","text":"<p>Jalapeno has been primarily developed, tested, and operated on:</p> <ul> <li>Ubuntu 18.04</li> <li>Ubuntu 20.04</li> <li>Ubuntu 22.04</li> </ul> <p>With the following Kubernetes environments (Bare-metal, VM, or Cloud):</p> <ul> <li>Kubernetes</li> <li>MicroK8s</li> </ul> <p>Recommended VM sizing for a test lab:</p> <ul> <li>4 vCPU</li> <li>16GB memory</li> <li>50G of disk.</li> </ul> <p>Tip</p> <p>If deploying in production or a test environment with large table sizes (full Internet table, 250k + internal or vpn prefixes), then we recommend a bare metal K8s cluster with two or more nodes.</p>"},{"location":"resources/","title":"Additional Resources","text":"<p>This section contains additional resources to help you get started with Jalapeno.</p> <p>Select a component below to get started:</p> <ul> <li> ArangoDB Sample Queries</li> <li> Influx Sample Queries</li> </ul> <p></p> Note <p>All provided examples in this section assume that you already have access to a running Jalapeno instance &amp; that instance is successfully receiving device telemetry.</p> <p>If this isn't the case, head back over to the Getting Started page.</p> <p></p>"},{"location":"resources/arangodb/","title":"Example Arango DB queries","text":"<p>This section will cover some example queries that can be run against the ArangoDB instance.</p> <p>Info</p> <p>Many of these queries are specific to a lab setup we run internally. A reference topology has been provided below.</p> <p>Please note, the queries will need to be adjusted to fit your environment / needs.</p> <p></p>"},{"location":"resources/arangodb/#link-state-collection-queries","title":"Link State Collection Queries","text":"<pre><code>for l in ls_node return l\nfor l in ls_node_edge return l\nfor l in ls_node_edge  return { from: l._from, to: l._to }\nfor l in ls_node filter l.router_id == \"10.0.0.8\" return l\nfor l in ls_node_edge filter l._key like \"%0019%\" return l\nfor l in ls_link filter l.mt_id_tlv.mt_id != 2 return l._key\nfor l in ls_link filter l.protocol_id == 7 &amp;&amp; l.peer_asn == 100000 &amp;&amp; l.remote_link_ip == \"10.71.0.1\" return { epe_sid: l.peer_node_sid.sid } \nfor l in ls_prefix filter l.prefix == \"10.0.0.8\" return l\nfor l in ls_prefix return l\nfor l in ls_prefix filter l.prefix_attr_tlvs.ls_prefix_sid != null return l\n\nfor l in ls_srv6_sid filter l.igp_router_id == \"0000.0000.0018\" for m in ls_srv6_sid filter m.igp_router_id == \"0000.0000.0017\" for n in ls_srv6_sid filter n.igp_router_id == \"0000.0000.0016\" for o in ls_srv6_sid filter o.igp_router_id == \"0000.0000.0021\" return [l.srv6_sid, m.srv6_sid, n.srv6_sid, o.srv6_sid]\n</code></pre>"},{"location":"resources/arangodb/#query-other-collections","title":"Query Other Collections","text":"<pre><code>for u in unicast_prefix_v4 return u._key\nfor l in unicast_prefix_v4 filter l.prefix == \"10.10.3.0\" filter l.base_attrs.as_path == Null return l\nfor d in peer filter d.remote_ip == \"10.72.0.1\"  for l in ls_link filter d.remote_ip == l.remote_link_ip return d\nfOR d IN peer filter d.remote_bgp_id == \"10.0.0.71\" filter d.remote_ip == \"10.71.0.1\" return d\nfor l in unicast_prefix_v4 return { key: l._key, prefix: l.prefix, nexthop: l.nexthop, as_path: l.base_attrs.as_path, origin_as: l.origin_as }\nfor p in unicast_prefix_v4 filter p._key == \"10.71.8.0_22_10.71.0.1\" return p\nfor l in l3vpn_v4_prefix filter l.base_attrs.ext_community_list like \"%100:100%\" return l//.base_attrs.ext_community_list\n</code></pre>"},{"location":"resources/arangodb/#shortest-path-queries","title":"Shortest path queries","text":"<p>(Optional) Add synthetic latency:</p> <pre><code>for l in ls_node_edge UPDATE l with { link_latency: 5 } in ls_node_edge\n\nfor l in ls_node_edge filter l._key == \"2_0_0_0_0000.0000.0003_10.1.1.19_0000.0000.0002_10.1.1.18\" UPDATE l with { link_latency: 30 } in ls_node_edge\n</code></pre> <p>Shortest Path:</p> <pre><code>for v, e in outbound shortest_path 'ls_node/2_0_0_0000.0000.0019' TO 'ls_node/2_0_0_0000.0000.0007' ls_node_edge return [v._key, e._key]\n\nfor v, e in outbound shortest_path 'ls_node/2_0_0_0000.0000.0019' TO 'ls_node/2_0_0_0000.0000.0007' ls_node_edge filter e.mt_id != 2 return e\n\nfor v, e in outbound shortest_path 'ls_node/2_0_0_0000.0000.0019' TO 'ls_node/2_0_0_0000.0000.0007' ls_node_edge filter e.mt_id != 2 return {node: v._key, link: e._key, latency: e.link_latency, asn: v.asn, local_asn: v.local_asn, remote_asn: v.remote_asn  } \n\nfor v, e in outbound shortest_path 'ls_node/2_0_0_0000.0000.0019' TO 'ls_node/2_0_0_0000.0000.0007' ls_node_edge OPTIONS {weightAttribute: 'link_latency' } filter e.mt_id != 2 return { node: v._key, link: e._key, latency: e.link_latency }\n</code></pre> <p>Get all paths up to X hops in length:</p> <pre><code>for v, e, p IN 1..6 OUTBOUND 'ls_node/2_0_0_0000.0000.0019' ls_node_edge FILTER v._id == 'ls_node/2_0_0_0000.0000.0007' RETURN CONCAT_SEPARATOR(\" -&gt; \", [p.vertices[*]._key, p.edges[*].link_latency])\n\nfor v, e, p IN 1..16 OUTBOUND 'ls_node/2_0_0_0000.0000.0019' lsv4_edge OPTIONS {uniqueVertices: \"path\", bfs: true} FILTER v._id == 'ls_node/2_0_0_0000.0000.0007' RETURN p.edges[*]._to\n</code></pre>"},{"location":"resources/influxdb/","title":"Sample InfluxDB Queries","text":"<p>This section will cover some example queries that can be run against the InfluxDB instance.</p>"},{"location":"resources/influxdb/#get-interface-names-ips","title":"Get Interface Names &amp; IPs","text":"<p>Provide all of Router 16's interface names and IPv4 addresses:</p> <pre><code>SELECT last(\"ip_information/ip_address\") FROM \"Cisco-IOS-XR-pfi-im-cmd-oper:interfaces/interface-xr/interface\" WHERE (\"source\" = 'R16-LSR') GROUP BY \"interface_name\"\n</code></pre>"},{"location":"resources/influxdb/#get-interface-ids","title":"Get Interface IDs","text":"<p>Provide Router 16's interface IDs or indexes:</p> <pre><code>SELECT last(\"if_index\") FROM \"Cisco-IOS-XR-pfi-im-cmd-oper:interfaces/interface-xr/interface\" WHERE (\"source\" = 'R16-LSR') GROUP BY \"interface_name\"\n</code></pre>"},{"location":"resources/influxdb/#get-txrx-bytes","title":"Get TX/RX Bytes","text":"<p>Provide transmit and receive bytes collected for a given router interface over the last hour (30 second collection interval)</p> <pre><code>SELECT last(\"state/counters/out_octets\"), last(\"state/counters/in_octets\") FROM \"openconfig-interfaces:interfaces/interface\" WHERE (\"name\" = 'GigabitEthernet0/0/0/0' AND \"source\" = 'R12-LSR') AND time &gt;= now() - 30m  GROUP BY time(30s) fill(null)\n</code></pre>"},{"location":"resources/influxdb/#get-mpls-bytes","title":"Get MPLS Bytes","text":"<p>Provide total MPLS label switched bytes for a given interface or label value</p> <pre><code>SELECT last(\"label_information/tx_bytes\") FROM \"Cisco-IOS-XR-fib-common-oper:mpls-forwarding/nodes/node/label-fib/forwarding-details/forwarding-detail\" WHERE (\"source\" = 'R12-LSR' AND \"label_information/outgoing_interface\" = 'Gi0/0/0/4')\n</code></pre> <pre><code>SELECT last(\"label_information/label_information_detail/transmit_number_of_bytes_switched\") FROM \"Cisco-IOS-XR-fib-common-oper:mpls-forwarding/nodes/node/label-fib/forwarding-details/forwarding-detail\" WHERE (\"source\" = 'R12-LSR' AND \"label_value\" = '100014')\n</code></pre>"},{"location":"resources/influxdb/#get-sr-traffic","title":"Get SR Traffic","text":"<p>Segment Routing Traffic Matrix collection</p> <pre><code>SELECT last(\"base_counter_statistics/count_history/transmit_number_of_bytes_switched\") FROM \"Cisco-IOS-XR-infra-tc-oper:traffic-collector/vrf-table/default-vrf/afs/af/counters/prefixes/prefix\" WHERE (\"source\" = 'R08-ABR' AND \"label\" = '100014') \n</code></pre>"},{"location":"usage/","title":"Jalapeno Usage","text":"<p>This section contains details for accessing &amp; interacting with a Jalapeno instance.</p> <p>Select a component below to get started:</p> <ul> <li> API</li> <li> ArangoDB</li> <li> Grafana</li> <li> InfluxDB</li> <li> Kafka</li> </ul>"},{"location":"usage/api/","title":"Jalapeno API","text":"<p>The Jalapeno API gateway is maintained under a separate project.</p> <p>Links:</p> <ul> <li>Jalapeno API gateway Repository</li> <li>Project Documentation</li> <li>Golang Client Library</li> <li>Python Client Library</li> </ul>"},{"location":"usage/arangodb/","title":"ArangoDB","text":"<p>ArangoDB is Jalapeno's graph database that serves as the central data-store for all network topology and performance data. It acts as the single source of truth regarding network state for Jalapeno Processors and the Jalapeno API Gateway.</p> <p>Nework devices are configured to send topology and performance data to Kafka. There, Jalapeno Processors parse through this data to create multiple \"virtual topologies\" that are stored in ArangoDB to represent the current state of the network.</p> <p>For example, the Topology Processor parses GoBMP messages and builds out collections such as <code>LSNode</code> and <code>L3VPNPrefix</code> in Jalapeno's ArangoDB instance.</p> <p>These collections, in conjunction with ArangoDBs rapid graphical traversals and calculations, make it easy to determine what path is optimal through the network given a specific SLA. For example, a user can request the <code>lowest-latency path</code> from point A to point B.</p>"},{"location":"usage/arangodb/#types-of-arangodb-interactions","title":"Types of ArangoDB Interactions","text":"<ul> <li> <p>Parsing data from Kafka and storing in ArangoDB (i.e. Topology Processor)</p> </li> <li> <p>Parsing data from ArangoDB and returning insights to client (i.e. API Gateway)</p> </li> </ul>"},{"location":"usage/arangodb/#deploying-arangodb","title":"Deploying ArangoDB","text":"<p>ArangoDB is deployed using <code>kubectl</code>, as seen in the <code>/install/infra/deploy_infrastructure.sh</code> script. The configurations for ArangoDB's deployment are in the YAML files in the <code>/infra/arangodb/</code> directory.  </p>"},{"location":"usage/arangodb/#accessing-arangodb","title":"Accessing ArangoDB","text":"<p>To access ArangoDB's UI, log in at <code>&lt;server_ip&gt;:30852</code>, using credentials <code>root/jalapeno</code>. In the list of DBs, select <code>jalapeno</code>.</p>"},{"location":"usage/arangodb/#querying-arangodb","title":"Querying ArangoDB","text":"<p>Sample queries have been provided in the Resources section.</p>"},{"location":"usage/grafana/","title":"Grafana","text":"<p>Grafana is Jalapeno's visual dashboard and metric-visualization tool.</p> <p>Jalapeno's data-pipeline populates the time-series database InfluxDB with metrics regarding the state of the network. Jalapeno's instance of Grafana loads InfluxDB as its data-source.</p> <p>Grafana can have any number of graphical representations of the health of the network, including historical bandwidth usage, historical latency metrics, and more.</p>"},{"location":"usage/grafana/#deploying-grafana","title":"Deploying Grafana","text":"<p>Grafana is deployed using <code>kubectl</code>, as seen in the <code>/install/infra/deploy_infrastructure.sh</code> script. The configurations for Grafana's deployment are in the various YAML files in the <code>/infra/grafana/</code> directory.  </p>"},{"location":"usage/grafana/#accessing-grafana","title":"Accessing Grafana","text":"<p>To access Grafana's UI, log in at <code>&lt;server_ip&gt;:30300</code>, using credentials <code>root/jalapeno</code>.</p>"},{"location":"usage/grafana/#creating-grafana-dashboards-for-jalapeno-telemetry-data","title":"Creating Grafana Dashboards for Jalapeno Telemetry Data","text":"<p>Jalapeno's installation script does not automatically add Grafana dashboards.</p> <p>To do so:</p> <ol> <li>Connect to Grafana: <code>http://&lt;jalapeno_ip&gt;:30300</code><ul> <li>User/pw = root/jalapeno</li> </ul> </li> <li>After authenticating, click on Add data source &amp; choose InfluxDB</li> <li>Enter InfluxDB parameters:<ul> <li>URL: <code>http://influxdb:8086</code></li> <li>Database: <code>mdt_db</code></li> <li>Basic Auth: <code>root/jalapeno</code></li> <li>HTTP method <code>GET</code></li> </ul> </li> <li>Click Save and Test</li> <li>Once added, hover over the Dashboard icon and click Manage</li> <li>Click Import on the right side of the screen</li> <li>Import telemetry json files and modify as necessary to fit your topology</li> </ol> <p>A few sample Grafana dashboards can be found here:</p> <ul> <li>Egress Traffic</li> <li>Ingress Traffic</li> </ul>"},{"location":"usage/influxdb/","title":"InfluxDB","text":"<p>InfluxDB is Jalapeno's time-series database.</p> <p>Network devices are configured to send performance data to Kafka. Then data is parsed using Telegraf (a telemetry consumer) and stored in InfluxDB. The data stored in InfluxDB can be queried by Jalapeno Processors.</p> <p>These queries construct and derive relevant metrics that inform Jalapeno's API Gateway responses to client requests. For example, a processor could generate rolling-averages of bytes sent out of a router's interface, simulating link utilization. Those calculated insights could then be inserted into ArangoDB and associated with their corresponding edges, providing a holistic view of the current state of the network.</p> <p>Using InfluxDB as a historical data-store, trends can also be inferred based on historical analysis. Processors and applications can determine whether instantaneous measurements are extreme anomalies, and can enable requests for any number of threshold-based reactions.</p>"},{"location":"usage/influxdb/#deploying-influxdb","title":"Deploying InfluxDB","text":"<p>InfluxDB is deployed using <code>kubectl</code>, as seen in the <code>/install/infra/deploy_infrastructure.sh</code>. The configurations for InfluxDB's deployment are in the various YAML files in the <code>/infra/influxdb/</code> directory.</p>"},{"location":"usage/influxdb/#accessing-influxdb","title":"Accessing InfluxDB","text":"<p>To access InfluxDB via Kubernetes, enter the pod's terminal and run:</p> <pre><code>influx\nauth root jalapeno\nuse mdt_db\nshow series #(1)!\n</code></pre> <ol> <li>Provides a list of all time-series in the mdt_db</li> </ol>"},{"location":"usage/influxdb/#querying-influxdb","title":"Querying InfluxDB","text":"<p>Sample queries have been provided in the Resources section.</p>"},{"location":"usage/kafka/","title":"Kafka","text":"<p>To access Kafka and validate Jalapeno topics and data:</p> <ol> <li> <p>Access container:</p> <pre><code>kubectl exec -it &lt;kafka pod name&gt; /bin/bash -n jalapeno\n</code></pre> </li> <li> <p>Change directory to <code>bin</code> and unset <code>JMX_PORT</code></p> <pre><code>cd bin\nunset JMX_PORT\n</code></pre> </li> <li> <p>List Jalapeno topics:</p> <pre><code>./kafka-topics.sh --zookeeper zookeeper.jalapeno.svc:2181 --list  \n</code></pre> </li> <li> <p>Listen for a topic's messages:</p> <pre><code>./kafka-console-consumer.sh --bootstrap-server localhost:9092  --topic gobmp.parsed.ls_node\n./kafka-console-consumer.sh --bootstrap-server localhost:9092  --topic gobmp.parsed.ls_link\n./kafka-console-consumer.sh --bootstrap-server localhost:9092  --topic gobmp.parsed.l3vpn_v4\n</code></pre> </li> </ol> <p>If the topic provides JSON output similar to the below sample, we know that GoBMP is successfully writing BMP messages to Kafka.</p> Note <p>There may not be active messages in Kafka's buffer at any given time.</p> <p>BMP messages can be triggered by either clearing BGP link-state or shut/no-shut on the router's BMP server.</p> <p>Sample output:</p> <pre><code>{\n  \"action\": \"add\",\n  \"router_hash\": \"9e1a9a3663f25a297ed16a834b473eb0\",\n  \"domain_id\": 0,\n  \"router_ip\": \"10.0.0.10\",\n  \"peer_hash\": \"308bc76d9c523fce904af3300c97d77e\",\n  \"peer_ip\": \"10.0.0.12\",\n  \"peer_asn\": 65000,\n  \"timestamp\": \"Nov  3 20:28:02.000896\",\n  \"igp_router_id\": \"0000.0000.0003\",\n  \"router_id\": \"10.0.0.3\",\n  \"asn\": 65000,\n  \"mt_id\": [\n    0,\n    2\n  ],\n  \"isis_area_id\": \"49.0901\",\n  \"protocol\": \"IS-IS Level 2\",\n  \"protocol_id\": 2,\n  \"node_flags\": 0,\n  \"name\": \"R03-LSR\",\n  \"ls_sr_capabilities\": {\n    \"sr_capability_flags\": 128,\n    \"sr_capability_tlv\": [\n      {\n        \"range\": 64000,\n        \"sid_tlv\": {\n          \"sid\": \"AYag\"\n        }\n      }\n    ]\n  },\n  \"sr_algorithm\": [\n    0,\n    1\n  ],\n  \"sr_local_block\": {\n    \"flags\": 0,\n    \"subranges\": [\n      {\n        \"range_size\": 1000,\n        \"label\": 15000\n      }\n    ]\n  },\n  \"srv6_capabilities_tlv\": {\n    \"flag\": 0\n  },\n  \"node_msd\": [\n    {\n      \"msd_type\": 1,\n      \"msd_value\": 10\n    }\n  ],\n  \"isprepolicy\": false,\n  \"is_adj_rib_in\": false\n}\n</code></pre>"}]}